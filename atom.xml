<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Meta-Interpretation]]></title>
  <link href="http://pfmiles.github.com/atom.xml" rel="self"/>
  <link href="http://pfmiles.github.com/"/>
  <updated>2012-07-15T01:13:29+08:00</updated>
  <id>http://pfmiles.github.com/</id>
  <author>
    <name><![CDATA[pf_miles]]></name>
    <email><![CDATA[miles.wy.1@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[LL(*)的概念与实现]]></title>
    <link href="http://pfmiles.github.com/blog/concept-and-implementation-of-ll-star"/>
    <updated>2012-07-14T13:59:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/concept-and-implementation-of-ll-star</id>
    <content type="html"><![CDATA[<h3>概念和起源</h3>

<p>LL(*)是由<a href="http://www.cs.usfca.edu/~parrt/">Terrence Parr</a>教授创造的，为使用<a href="http://en.wikipedia.org/wiki/LL_parsing">LL分析</a>的语法解析器做超前查看(look ahead)的一种算法。按照Parr教授的说法，这个算法从最初的想法至今的完善和调整已经历了15年。目前该算法的一个著名的实现在<a href="http://www.antlr.org/">antlr3</a>中。</p>

<h3>特点，n言以蔽之&#8230;</h3>

<ul>
<li>对于语法产生式分支的预测判断，走LL分析路子的解析器，其能力完全取决于能超前查看多少个token。传统地讲，LL(1), LL(2)直至LL(k)，就是在讲该解析器能够在语法分析过程中超前查看1, 2, k&#8230;个token。</li>
<li>而LL(*)的意思就是，它在语法解析的过程中，超前查看的token个数不是固定的，是可伸缩的，不过这一点LL(k)分析也能做到(在k范围之内&#8230;)；</li>
<li>但是，LL(*)还能越过一些重复出现的token，“到达”这些重复出现的token之后的token来做分析，这一点LL(k)是无法办到的(LL(k)无法意识到有token在循环出现，不管情况如何，它都将在尝试k个token之后放弃)；</li>
<li>如果将超前查看的决策逻辑画成DFA的话，就是这样的一种形式：<img src="http://pfmiles.github.com/images/ll3.png" title="Look Ahead DFA" alt="Look Ahead DFA" />

<ul>
<li>这张图画的是这种语法规则的超前查看决策情况：<code>A ::= a b c | a b e</code>, 显然这是一个LL(3)语法；</li>
<li>但是这样的语法: <code>A ::= a b* c | a b* e</code>是无法被LL(k)识别的，因为中间的<code>b*</code>代表“0个或多个b”(kleene闭包)，这并不是一个固定的重复次数，因此LL(k)无法识别，for any k&#8230;</li>
<li>但是，LL(*)算法能够构造出这样的DFA来识别它：<img src="http://pfmiles.github.com/images/llstar.png" title="LL(*) Look Ahead DFA" alt="LL(*) Look Ahead DFA" /></li>
<li>也就是说，传统LL(k)的look ahead DFA是不带环的，而LL(*)算法能构造出带环的DFA来做判断，它能越过无穷多个存在于环上的token，从而“到达”环之后的token继续做判断。</li>
</ul>
</li>
<li>上面的<code>A ::= a b* c | a b* e</code>语法使用了<a href="http://pfmiles.github.com/blog/left-recursion-removal-using-kleene-closure/">&#8220;kleene闭包&#8221;表示法</a>来表示“0个或多个”，这种表示法在正则表达式中很常见；事实上，基于LL(*)算法实现的语法解析器生成器(比如antlr3)对Kleene闭包表示法特别友好，可以鼓励使用，还能顺便解决一些LL分析法所不允许的“<a href="http://en.wikipedia.org/wiki/Left_recursion">左递归</a>”。</li>
<li>大多数情况来讲，LL(*)的识别能力弱于<a href="http://en.wikipedia.org/wiki/LR_parser">LR(k)</a>，但也有少数情况是LR(k)识别不了但LL(*)可以的，所以LL(*)与LR(k)之间并没有严格的强弱顺序。</li>
<li>LL(*)本身对语法的识别能力也是有限的，比如它无法区分&#8221;具有相同的递归规则前缀&#8221;的多个分支，这种情况是要尽量避免的，大多数能够较容易地改写;</li>
<li>对于LL(*)不能正确分析的情况，还能引入&#8221;Semantic Predicate&#8221;(语义断言)来辅助判断, Semantic Predicate可以是任何逻辑，只要返回一个bool值就行；有了Semantic Predicate的辅助，LL(*)甚至能够parse一些上下文相关的语法，实际上它能parse C++</li>
</ul>


<h3>主要算法及其实现(python)</h3>

<p><strong> Parr教授在2011年发表的paper&#8221;LL(*): The Foundation of the ANTLR Parser Generator&#8221;中详细描述了这个算法，下面就是其主要过程，具体的代码实现(python)在工程<a href="https://github.com/pfmiles/llstar">llstar</a>中 </strong></p>

<!-- more -->


<p>为了简单起见，约定大写字母用来命名non-terminal, 小写字母命名terminal或Semantic Predicate，<code>$</code>表示EOF，假设有如下语法规则：</p>

<pre><code>S ::= A $  
A ::= a c*  
    | b c*  
</code></pre>

<ul>
<li>针对此语法规则，构造Augmented Transition Network(ATN)，其实也就是一个NFA，NFA的边就是terminal或non-terminal或者Semantic predicate：<img src="http://pfmiles.github.com/images/atn.png" alt="ATN network" />, 代码在llstar工程的<code>atn_creation.py</code>的<code>rule.merge_to_atn</code>方法中。</li>
<li>接下来将要对这个ATN实施的算法其实就是经典的<a href="http://en.wikipedia.org/wiki/Powerset_construction">Subset Construction算法</a> —— 的改造版本：在“空边闭包运算(closure)”中加入了压栈、出栈功能，用于处理运算过程中<code>closure</code>函数经过non-terminal边时对其所对应的ATN的调用和返回。<br/>
这个算法的具体描述是比较复杂，在Parr教授的paper中已有精确描述。不过真的要用代码实现起来，某些细节还需稍微修饰，才能正确地实现。<br/>
最后的结果，其实就是使用更改后的版本的Subset Construction算法，将上述ATN(NFA)转化为一个个DFA，每条语法规则都对应一个DFA作为其look ahead DFA。<br/>
例如上面的<code>S</code>规则最后的DFA为：<img src="http://pfmiles.github.com/images/sdfa.png" alt="S DFA" />, well, 就一个终结态，没任何状态转换 —— 这是当然的，S规则只有一个alternative production&#8230; <br/>
而<code>A</code>规则的DFA为：<img src="http://pfmiles.github.com/images/adfa.png" alt="A DFA" />, 意思就是，如果第一个token为<code>a</code>，就选择第一个分支，若是<code>b</code>，就选择第二个分支，这是一个LL(1)的语法。<br/>
简单来说就是这样，具体可以参考llstar的代码&#8230;这花了我不少时间调试出来的代码，还是直接看代码了解最直接:)</li>
</ul>


<h3>实验台及例子介绍</h3>

<p>OK，主要的算法原理了解之后，展示了2个玩具例子，似乎还不能看出LL(*)的特点来。<br/>
下面我将逐步展示一些真正non-trivial的例子，以表明它真的不是过家家&#8230;</p>

<p>LL(3)语法(对应llstar工程中的<code>LL_3_many_alts.py</code>):</p>

<pre><code>S ::= A $
A ::= B a*
    | C a+
    | D a?
B ::= a b c C
    | a b c D
    | d
C ::= e f g D
    | e f g h
D ::= i j k l
    | i j k m
</code></pre>

<p>这个语法是固定的LL(3)，没有什么特别，不过分支比较多, 它的ATN：<img src="http://pfmiles.github.com/images/ll3atn.png" alt="LL(3) ATN" /><br/>
规则<code>A</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/ll3adfa.png" alt="LL(3) Rule A DFA" /><br/>
规则<code>B</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/ll3bdfa.png" alt="LL(3) Rule B DFA" /><br/>
规则<code>C</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/ll3cdfa.png" alt="LL(3) Rule C DFA" /><br/>
规则<code>D</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/ll3ddfa.png" alt="LL(3) Rule D DFA" /></p>

<p>上面这个语法，LL(4)分析也能搞定，而真正体现出LL(*)特点的是下面这样的语法(对应llstar工程中<code>over_kleene.py</code>)：</p>

<pre><code>S ::= A $
A ::= a* b
    | a+ c
</code></pre>

<p>其中，规则<code>A</code>的2条分支拥有共同的kleene闭包前缀，LL(k)是无法识别的，它的ATN:<img src="http://pfmiles.github.com/images/llstar/overkleeneatn.png" alt="Over Kleene ATN" /><br/>
而LL(*)能够为<code>A</code>生成这样的DFA：<img src="http://pfmiles.github.com/images/llstar/overkleeneadfa.png" alt="Over Kleene Rule A DFA" /><br/>
这个DFA能够越过terminal<code>a</code>的任意重复，从而考虑到后面的terminal<code>b</code>和<code>c</code>来做判断<br/>
而下面这个更加&#8221;恶劣&#8221;的列子则更能说明LL(*)的这个特征(对应llstar工程中<code>over_non_recurse_rule.py</code>)：</p>

<pre><code>S ::= A $
A ::= B* b
    | C+ c
B ::= a d
    | e
C ::= a d
    | e
</code></pre>

<p>这个例子中，<code>B</code>和<code>C</code>完全相同，并分别被包含在规则<code>A</code>的2条分支的kleene闭包中，<br/>
最终生成的<code>A</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/nonrecursiveradfa.png" alt="Non-recursive Rule A DFA" /></p>

<p>上面提到过，LL(*)也有不能解决的情况，比如2条分支拥有&#8221;共同的、递归的规则作为前缀&#8221;，比如这样：</p>

<pre><code>S ::= A eof
A ::= E a
    | E b
E ::= c E d
    | e
</code></pre>

<p>上面的<code>A</code>规则的两条分支拥有共同的前缀<code>E</code>，而<code>E</code>本身是一条递归规则，这样的规则是LL(*)无法分析的，在llstar工程中对应<code>experiments/common_recurse_prefix_fail.py</code>, 运行的时候会抛出错误提示。这种情况在antlr3中会将parsing策略退化为LL(1) + 回溯的形式。</p>

<pre><code>Traceback (most recent call last):
  File "/home/pf-miles/myWorkspace/llstar/experiments/common_recurse_prefix_fail.py", line 31, in &lt;module&gt;
    d_a = create_dfa(ra.get_start_state(globals_holder.a_net))
  File "/home/pf-miles/myWorkspace/llstar/algos.py", line 136, in create_dfa
    d_state_new.add_all_confs(closure(d_state, conf))
  File "/home/pf-miles/myWorkspace/llstar/algos.py", line 99, in closure
    raise Exception("Likely non-LL regular, recursive alts: " + str(d_state.recursive_alts) + ", rule: " + str(globals_holder.a_net.get_rule_of_state(p)))
Exception: Likely non-LL regular, recursive alts: set([0, 1]), rule: E
</code></pre>

<p>还有一些特殊情况，比如规则完全就是冲突的，那么这个时候就是Semantic Predicate发挥作用的时候(对应llstar工程中<code>conflicting_with_preds.py</code>)：</p>

<pre><code>S ::= A $
A ::= {pred1}? B
    | {pred2}? b
B ::= b
    | b
</code></pre>

<p>最终<code>A</code>的DFA：<img src="http://pfmiles.github.com/images/llstar/confresolvedwithpredsadfa.png" alt="Conflict Resolved With Preds A DFA" /><br/>
实际开发中，写出这样的规则是坑到爷的&#8230;不过LL(*)还算能很好解决&#8230;其实这里就算没有Semantic Predicate, LL(*)也能按照convention，“选择较早定义的语法分支”来解决。</p>

<p>还有就是在分析的过程中会导致分析栈溢出(不是程序语言的callstack的overflow, 而是LL(*)的closure操作维护的一个规则调用栈溢出)的时候，LL(*)也会使用Semantic Predicate来辅助判断, 比如规则(对应llstar中<code>overflow_with_preds.py</code>):</p>

<pre><code>S ::= A $
A ::= {pred1?} B a
    | {pred2?} b* c
    | {pred3?} b* c
B ::= b+ B
</code></pre>

<p>规则<code>A</code>的三条分支全部存在严重冲突(都拥有能识别无穷个&#8217;b&#8217;的前缀)并且其中一个是另一条递归规则<code>B</code>，这种形式会导致LL(*)的分析栈溢出，不过仍然没关系，这样的情况能在溢出发生时，调用<code>pred1</code>、<code>pred2</code>和<code>pred3</code>来解决(这三个pred就是Semantic Precicate，是能包含任何逻辑但最终返回bool值的表达式)<br/>
生成的规则<code>A</code>的DFA: <img src="http://pfmiles.github.com/images/llstar/overflowadfa.png" alt="Overflow A DFA" /><br/>
这里设定的最大分析递归层数为<code>10</code>，因此，DFA在接受了第10个&#8217;b&#8217;之后达到了溢出状态<code>d14</code>，这时调用了各个alternative附带的几个predicate来解决冲突&#8230;</p>

<h3>总结</h3>

<p>OK, 大概如是了；上述算法的实现、例子的代码均在<a href="https://github.com/pfmiles/llstar">llstar</a>工程中能找到<br/>
llstar工程是我花了不少时间实现并调试好的一块LL(*)算法的“实验台”，可以在里面慢慢把玩各种变态的、五花八门的语法规则，以验证LL(*)的分析能力<br/>
在工程的<code>experiments</code>文件夹下已经有许多具有代表性的例子&#8230;都是python代码<br/>
另外，llstar工程是在eclipse + pydev环境下开发的，因此如果要在命令行里运行，可能要对例子里面的import路径稍作修改，当然，最好是直接在eclipse + pydev环境下运行它们了<br/>
在linux环境下，llstar的所有例子都能自动生成上文中看到的各种DFA/NFA图像，不过前提是环境中要装有<a href="http://www.graphviz.org/">graphviz</a></p>

<p>现在的所有语法解析器生成器中，似乎都是LR分析的天下，LL分析大多是手写解析器的首选&#8230;<br/>
不过在现有的使用LL分析方式的产品中，也就只有LL(*)和PEG(Packrat)parsing比较常见也比较实用了，能与LR分析一较高下。<br/>
而配上回溯策略的LL(*)是严格强于Packrat parsing的，这样看来大概走LL分析路子的自动化工具也只有LL(*)容易一枝独秀了(想必这也是antlr相对较为流行的原因)。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Algo-class第二周小记: Master Method]]></title>
    <link href="http://pfmiles.github.com/blog/algo-class-week2-notes-master-method"/>
    <updated>2012-04-03T11:40:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algo-class-week2-notes-master-method</id>
    <content type="html"><![CDATA[<p>week2主要讲了master method，感觉从来没有听过如此清晰有条理的介绍master method的讲解，不得不佩服<a href="http://theory.stanford.edu/~tim/">Tim Roughgarden</a>教授的功力。<br/>
从一些随课笔记总结起来，master method是用来预估divide and conquer算法的大O时间复杂度的公式；它要求divide的时候“所有sub-problems的规模都相等”才能适用。<br/>
要使用master method, 首先要将算法的时间消耗函数写成递推公式形式：
$$T(n) &lt;= aT(\frac{n}{b}) + O(n^d)$$
其中T(n)是本层递归所消耗的时间，$T(\frac{n}{b})$自然就是下一层递归所消耗的时间了，b就是每次往“下层”递归调用时问题规模缩小的倍率，a就是代码中递归调用的个数；$O(n^d)$ 是将递归调用之外的其它代码的时间开销写成关于输入规模n的指数形式，d就是这个指数。<br/>
基础情况： 当n足够小时，T(n) &lt;= 一个常数<br/>
a,b,d与输入规模无关。<br/>
那么，根据a,b,d的情况，T(n)可能被表示为三种情况：<br/>
<img src="http://pfmiles.github.com/images/masterMethod.png" title="Master Method" alt="Master Method" /></p>

<!-- more -->


<p>对于这个式子，我个人的、不严格的、直觉的理解就是：</p>

<ol>
<li>当$a = b^d$时，程序规模被divide之后，合并代价增大的幅度与程序规模缩小的幅度相当，所以复杂度公式中合并代价部分和切分、解决部分的复杂度都要算在里面</li>
<li>当$a &lt; b^d$时，合并代价增长快于切分、解决缩小幅度，所以复杂度公式中只算合并代价部分</li>
<li>当$a > b^d$时，切分、解决部分时间代价占主导，忽略递归调用以外的项，只写出divide and conquer递归调用部分的时间开销</li>
</ol>


<p>不过其实这个是有严格的证明的：<br/>
设$T(n) = aT(\frac{n}{b}) + cn^d$<br/>
对于第j层递归 ，运算量为：$a^j \times c \times (\frac{n}{b^j})^d$<br/>
即：$cn^d(\frac{a}{b^d})^j$，所以a和$b^d$的大小关系是关键！<br/>
进而总计算时间： $total work \leqslant cn^d \centerdot \sum\limits_{j=0}^{\log_bn}(\frac{a}{b^d})^j$, 然后通过这个式子，由a和$b^d$的大小关系可以较容易想通为什么master method会是上述这种三段分段函数的形式。<br/>
Week2作业是快排&#8230;写个快排倒是简单，可是作业要求要count快排过程中的比较次数&#8230;在作业论坛里面看到哀嚎声一片&#8230;毕竟实现只要稍微有点偏差，虽然同样是快排，确实很容易比较次数不一样。<br/>
值得一提的是讲解中给出的in-place的快排partition方法，就是不用建立额外的数组来进行partition，python示意代码如下：</p>

<pre><code>def partition(pivotIndex, arr):
    swap(arr, 0, pivotIndex)
    pivot = arr[0]
    i = 1 # index 0...i &lt; pivot
    j = 1 # index i...j &gt; pivot
    for k in range(1, len(arr)):
    if arr[k] &lt;= pivot :
        swap(arr, i, k)
        i += 1
        j += 1
    else:
        j += 1
    return (arr[1:i], pivot, arr[i:j])
</code></pre>

<p>另外，从快排的例子可以看出，若divide and conquer算法是基于某种比较结果，将问题规模初步divide and conquer的话，每一次比较“能否将问题划分为规模差不多均等的子问题”将很大地影响算法实际的执行效率。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Kleene闭包语法糖消除语法中的左递归]]></title>
    <link href="http://pfmiles.github.com/blog/left-recursion-removal-using-kleene-closure"/>
    <updated>2012-03-28T11:18:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/left-recursion-removal-using-kleene-closure</id>
    <content type="html"><![CDATA[<p>一般来讲，如果使用的解析器生成器是基于LL分析法来解析文本的话(比如antlr)，我们就需要在编写语法规则的时候避免<a href="http://en.wikipedia.org/wiki/Left_recursion">左递归</a>。<br/>
虽然有一套成熟的、公式化的方法来去除左递归，但它会影响原有文法的结合顺序、优先级，或者多出一些本不需要的、额外的产生式。<br/>
例如，有左递归语法规则：</p>

<pre><code>addition ::= addition '+' factor
           | factor
</code></pre>

<p>这个式子其实是想要表达一个“一连串+号组成的和”的表达式，例如<code>1+2+3+4</code>；<br/>
假设目前的输入就是<code>1+2+3+4</code>, 那么超前查看带来的选择应该是<code>addition '+' factor</code>这个产生式；于是在parser的代码中立刻就又直接调用了<code>addition()</code>方法，但注意，整个递归过程没有消耗掉任何token&#8230;也就是说，当流程再次递归到<code>addition</code>函数里时，输入<code>1+2+3+4</code>并没有任何改变，这又会进入下一轮递归&#8230;很快就会overflow。<br/>
这个问题，除了机械性地按照公式来消除左递归外，还可以改写语法规则，使其成为右递归语法：</p>

<pre><code>addition ::= factor '+' addition
           | factor
</code></pre>

<p>这样就不用stackOverFlow了，但是带来另一个问题：这使得我们的&#8217;+&#8217;加法成为了一个&#8217;右结合&#8217;的运算&#8230;<br/>
也就是说，<code>1+2+3+4</code>的计算次序是这样的：<code>1+(2+(3+4))</code><br/>
还好这里是“加法”，整数集合和加法运算构成“阿贝尔群(可交换群)”，满足交换律，所以就算实现成右结合的也能蒙混过关&#8230;但减法呢？除法呢？</p>

<!-- more -->


<p>难道要先生成一个AST，再写个visitor，把节点顺序给rewrite一下，然后再写个visitor求值？这么麻烦就为了实现一个简单的四则运算功能也太坑爹了。</p>

<p>其实，我们在这里使用递归，只是为了表达元素的“重复”这种意思；的确，递归能表示“重复”的概念，但这很容易让人联想到“循环”，因为平时的编程中，我们都通常使用循环来表达“重复”这种东西的。<br/>
更重要的是，循环与<a href="http://en.wikipedia.org/wiki/Tail_call">尾递归</a>在逻辑上是等价的，这使得我们可以将上述左递归改写成某种表示“循环”的形式，在这里，<a href="http://en.wikipedia.org/wiki/Kleene_star">Kleene Closure(克林闭包)</a>就正是一种表达“循环”的标记形式。<br/>
熟悉正则表达式的我们都知道，符号<code>*</code>(kleene star)和符号<code>+</code>(kleene cross)分别用来表示前面临接元素的“0个或多个”和“一个或多个”，如果这种标记用在我们的语法规则中那就酷毙了：</p>

<pre><code>addition ::= (factor '+')* factor
</code></pre>

<p>表达的意义和&#8221;左递归版本&#8221;的语法规则是一样的，但是，这种记法的重大意义就是：在生成的parser程序中可以直接以一个<code>while</code>语句来解析重复的<code>(factor '+')</code>而不用再递归回<code>addition</code>中！这样就避免了parser中的左递归实现带来的问题！<br/>
由于kleene闭包所能表达的所有语法规则形式都能被递归所表达，所以引入kleene闭包并没有从形式上增加语法解析器生成器的表达能力，所以说kleene闭包只是一个“语法糖”。<br/>
但它的实现意义却是重大的：解决了“为了表达‘元素重复’”的这一类左递归问题。
当然，<a href="https://github.com/pfmiles/dropincc.java">dropincc.java</a>也会支持kleene闭包的表达形式，大致的api形式如下：</p>

<pre><code>addition.fillGrammarRule(addend, CC.ks(ADD.or(SUB), addend));
</code></pre>

<p><code>CC.ks</code>方法就是kleene star的意思，当然还有<code>CC.kc</code>，顾名思义。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[algo-class]第一周作业——Count Inversions]]></title>
    <link href="http://pfmiles.github.com/blog/algo-class-week-one-assignment-counting-inversions"/>
    <updated>2012-03-25T15:41:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algo-class-week-one-assignment-counting-inversions</id>
    <content type="html"><![CDATA[<p><a href="http://algo-class.org">algo-class</a>还是挺有意思的，一上来就讲merge sort；介绍了分治法以及&#8221;piggyback&#8221;的思路&#8230;当作是复习吧：</p>

<h3>分治法(divide and conquer)</h3>

<ol>
<li>divide into smaller problems</li>
<li>conquer sub-problems recursively</li>
<li>combine solutions of problems into one for the original problems</li>
</ol>


<p>而&#8221;piggyback&#8221;的意思就是：记住一些经典算法的实现，遇到类似问题的时候，可以将要解决的问题“搭载”到那些经典算法上面，一般都具有和那些经典算法相同的平均时间复杂度；相当于是把那些经典算法当作一个个“模式”或者“骨架”了(所以说平时经常听到的经典算法还是要熟悉熟悉的，很多地方能派上用场)。</p>

<p>这一周围绕merge sort来讲divide and conquer真是再合适不过了；而作业“Count Inversions”就是一个将场景piggyback到merge sort上的一个典型例子；  作业给出了一个包含10W行数据的文本文件，每一行一个随机数字，要求找出这10W个数字中的所有Inversions。我用python实现如下：</p>

<pre><code>def countInversions(arr):
    length = len(arr)
    if length == 0 or length == 1:
    return (0, arr) 
    mid = int(length / 2)
    (leftCount, leftSortedArr) = countInversions(arr[0:mid])
    (rightCount, rightSortedArr) = countInversions(arr[mid:length])
    (mergedCount, mergedSortedArr) = mergeCount(leftSortedArr, rightSortedArr)
    return (leftCount + rightCount + mergedCount, mergedSortedArr)

def mergeCount(lArr, rArr):
    count = 0
    mergedArr = []
    i = 0
    j = 0
    lenl = len(lArr)
    lenr = len(rArr)
    while i &lt; lenl and j &lt; lenr:
    if lArr[i] &gt; rArr[j]:
        count += lenl - i # key line
        mergedArr.append(rArr[j])
        j += 1
    else:
        mergedArr.append(lArr[i])
        i += 1
    if i &lt; lenl:
    mergedArr.extend(lArr[i:lenl])

    if j &lt; lenr:
    mergedArr.extend(rArr[j:lenr])
    return (count, mergedArr)

import sys
if len(sys.argv) &lt; 2:
    print 'usage: python countInversionInAFile path_to_file'
    exit(1)
numbers = [int(line) for line in open(sys.argv[1], 'r')]
print countInversions(numbers)[0]
</code></pre>

<p>程序或许想起来简单但实际上要正确还是要费点功夫的；有些地方没想到的还非得debug才能看清楚为什么，比如程序中标注为&#8221;key line&#8221;那里，本来我写的是<code>count += 1</code>的但怎么都不对，后来debug才看出来应该写<code>count += lenl - i</code>。</p>

<p>另外，本来打算跟2门课程的，但现在看来我这忙碌的程序员完全没有时间&#8230;只好放弃“机器人汽车”那门课目前专门跟这一门了，毕竟“算法”的可操作性要强一些&#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[dropincc.java API design]]></title>
    <link href="http://pfmiles.github.com/blog/dropincc-dot-java-api-design"/>
    <updated>2012-03-18T16:29:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/dropincc-dot-java-api-design</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/pfmiles/dropincc.java">dropincc.java</a>为了使得用户能够在java语言中直接定义新语言的词法、语法规则，API设计采用了串接与组合(cascading &amp; composition)风格的<a href="http://martinfowler.com/bliki/FluentInterface.html">FluentInterface</a>形式。</p>

<p>这种形式能够使得这一套API看起来更具有“业务语义”，而不需要像antlr等其它工具一样强迫用户去学习全新的另外一种含有业务语义的DSL。<br/>
新语言的词法、语法规则被用户直接在java语句里书写、执行，其实得到的是dropincc.java这个CC工具的内部概念的AST。这个AST其实与“让用户学习一套DSL，然后编写，然后编译这些DSL得到的AST”是一样的。<br/>
也就是说 —— dropincc.java里面很重要的一个理念就是：像书写字面量一样书写CC工具的AST(AST literal???)。</p>

<blockquote><p>注意：强调一下这里说的AST并非用户想要实现的新语言的AST，我指的是dropincc.java这个工具其内部表达词法、语法规则的内部形式。</p></blockquote>

<p>经过一些初步的尝试，我认为在java中定义出这样一套API是完全可行的，其中一些值得列出的、具有代表性的点如下：</p>

<!-- more -->


<ol>
<li>词法规则强制用户限定在正则文法的表达能力之内(即使用正则表达式，例子略)</li>
<li>上下文无关文法的表示主要是解决3种形式的表示：连接、选择、递归;

<ol>
<li>其中“连接”的表示, 若BNF为：<code>term ::= LEFTPAREN expr RIGHTPAREN</code>, 则右边的部分在java中表达为：<code>func(LEFTPAREN, expr, RIGHTPAREN)</code>;即：“以方法传参的自然按顺序排列表达‘连接’的意思”;而其中<code>expr</code>是一个non-terminal，也就是说这里要引用<code>expr</code>这个规则，这也就表达了“递归” —— 要递归地引用<code>expr</code>的话那么直接写在这里就好了；</li>
<li>“选择”的表示，即一个产生式有好几个alternative的情况，若BNF是<code>term ::= DIGIT multail | LEFTPAREN expr RIGHTPAREN | DIGIT;</code>, 则java中表达为：<code>func(DIGIT, mulTail).alt(LEFTPAREN, expr, RIGHTPAREN).alt(DIGIT);</code>。</li>
</ol>
</li>
<li>上述3种基本元素的表达具备之后，其实已经足够表达出复杂的语法了，不过为了“更好的体验”，为了用得更方便，我认为还应该加入“子rule语法糖”。即让使用者能够以“加括号”的方式在产生式中直接定义子规则，而不必每次都单独写成一个产生式，这种做法应该能很受欢迎，比如：

<ul>
<li>第一个产生式是：<code>MUL_DIV ::= MUL | DIV;</code></li>
<li>第二个产生式是：<code>mulTail ::= MUL_DIV term;</code></li>
<li>其实，若是有了前面说的语法糖，那么上述2个产生式就可以合并为：<code>mulTail ::= (MUL | DIV) term;</code>, 少了<code>MUL_DIV</code>这个没太多意义的产生式定义；</li>
<li>在java中表达为：<code>func(MUL.or(DIV), term)</code>; 若子规则中想表达的不是“选择”关系，而是“连接”关系的话，也就是<code>mulTail ::= (MUL DIV) term;</code>的话，在java中表达为：<code>func(MUL.and(DIV), term)</code>; 其中<code>and</code>这个方法就是表达“连接”关系，只不过加了括号，成为了一个子规则而已。</li>
</ul>
</li>
</ol>


<p>OK, dropincc.java的核心FluentInterface形式的API应该就这些了，我相信这是一种很自然的方式，用户需要掌握的东西很少。</p>

<h3>设计FluentInterface风格API过程中的经验总结</h3>

<p>对于这个接口设计，我的经验就是：</p>

<ol>
<li>传入参数尽量“泛化”、“统一接口化”;这个“统一接口”最好不要包含任何方法(也就是说只是一个标记接口)，否则会给FluentInterface带来一些令人困惑的方法选项(当用户敲入&#8217;.&#8217;操作符的时候将会在IDE工具的方法提示里多出很多不必要的方法)。</li>
<li>每个串接方法的返回结果可以随意“具体化”，因为这个返回结果的具体类名是不必展示给用户看的，所以你打算让用户下一步还能串接出什么样的方法，就尽管去返回一个具体的子类；但要注意返回结果不应该有太深的继承层次，否则“下一步串接”(当用户敲入&#8217;.&#8217;操作符的时候)将会在IDE工具里列出令人困惑的方法列表 —— 一般就一层继承关系 —— 继承自Object —— 一个接口实现 —— 就是1中所述的“泛化接口”，这样做方便你将返回值继续传入另一个方法中 —— 实现为所欲为的cascading &amp; composition&#8230;</li>
</ol>


<h3>以闭包的形式嵌入动作代码</h3>

<p>几乎所有的CC工具都提供一种“在match到特定的节点或树模式时允许执行一段用户自定义代码”的功能；这种自定义代码我把它叫做“动作代码”, “action”。无论是lex &amp; yacc或是antlr都是这样。<br/>
这个功能几乎是必须的，这提供了一种自由度相当高的形式让用户可以自定义AST节点，或是直接在parse的过程当中做一些自定义的计算，然后当整个parse结束时直接输出的就是整个的计算结果。<br/>
动作代码一般来讲，都是用该CC工具所要生成的程序所使用的语言来写的；比如lex要生成C代码，他们的动作代码就是C来写的；antlr要生成java代码，那么它的动作代码自然就是java写的（当然antlr也能生成其它语言代码，相对应的，就使用那种语言来编写动作代码）。
dropincc.java既然设计目标是要嵌入到java语言中去使用，自然就是要用java语言来写动作代码了；<br/>
但是，跟lex &amp; yacc或者antlr不同的是，我并不打算让用户写一些残缺不全的动作代码放在某个约定的位置然后在生成目标程序的过程中将它们“编织”到目标程序中，这样的话就跟传统的CC工具没有任何区别了，没有什么实质上的改进；<br/>
既然dropincc.java是“嵌入”到普通的java应用程序代码中间工作的，那么它完全有理由使用“闭包(closure)”的形式来组织“动作代码”！为什么用“闭包”？因为dropincc.java是“嵌入”到业务代码中使用的，用闭包来写动作代码可以capture一些业务代码上下文信息，这对构建DSL来讲是非常大的便利！<br/>
比如，你的业务代码上下文里面有个<code>OrderService</code>对象，它负责一些&#8221;订单&#8221;业务逻辑，而当你编写一个闭包作为动作代码的时候，你完全可以&#8221;capture&#8221;上下文中的这个<code>OrderService</code>对象，从而使得你创造的语言在执行逻辑上与订单业务紧密、无缝地结合 —— 这是一种非常平滑的构建DSL的方式。</p>

<h3>最终的样子</h3>

<blockquote><p>注：此段代码只代表示意的风格、API情况，虽然能编译通过、运行，但不一定是行为完全正确四则运算表达式计算器</p></blockquote>

<pre><code>// 3.define lexical rules
Lang calculator = new Lang();
Token DIGIT = calculator.addToken("\\d+");
Token ADD = calculator.addToken("\\+");
Token SUB = calculator.addToken("\\-");
Token MUL = calculator.addToken("\\*");
Token DIV = calculator.addToken("/");
Token LEFTPAREN = calculator.addToken("\\(");
Token RIGHTPAREN = calculator.addToken("\\)");
// 2.define grammar rules and corresponding actions
Grule expr = new Grule();
Grule term = new Grule();
Element mulTail = calculator.addGrammarRule(MUL.or(DIV), term).action(
        new Action() {
            public Object act(Object... params) {
                return params;
            }
        });
term.fillGrammarRule(DIGIT, mulTail).action(new Action() {
    public Object act(Object... params) {
        int factor = Integer.parseInt((String) params[0]);
        Object[] mulTailReturn = (Object[]) params[1];
        String op = (String) mulTailReturn[0];
        int factor2 = (Integer) mulTailReturn[1];
        if ("*".equals(op)) {
            return factor * factor2;
        } else if ("/".equals(op)) {
            return factor / factor2;
        } else {
            throw new RuntimeException("Unsupported operator: " + op);
        }
    }
}).alt(LEFTPAREN, expr, RIGHTPAREN).action(new Action() {
    public Object act(Object... params) {
        return params[1];
    }
}).alt(DIGIT).action(new Action() {
    public Object act(Object... params) {
        return Integer.parseInt((String) params[0]);
    }
});
Element addendTail = calculator.addGrammarRule(ADD.or(SUB), term)
        .action(new Action() {
            public Object act(Object... params) {
                return params;
            }
        });
expr.fillGrammarRule(term, addendTail, CC.EOF).action(new Action() {
    public Object act(Object... params) {
        int addend = (Integer) params[0];
        Object[] addendTailReturn = (Object[]) params[1];
        String op = (String) addendTailReturn[0];
        int addend2 = (Integer) addendTailReturn[1];
        if ("+".equals(op)) {
            return addend + addend2;
        } else if ("-".equals(op)) {
            return addend - addend2;
        } else {
            throw new RuntimeException("Unsupported operator: " + op);
        }
    }
});
// 1.compile it!
calculator.compile();
// 0.FIRE!!!
System.out.println(calculator.exe("1+2+3+(4+5*6*7*(64/8/2/(2/1)/1)*8+9)+10"));
</code></pre>

<p>上面约60行java代码实现了一个相当复杂的多项式计算器语言：支持加减乘除四则运算以及括号调整优先级。而以往我们使用antlr来实现这个功能的话，最终生成、放到项目中工作的java代码会以千行计。</p>

<ul>
<li>这段代码是能够完全嵌入到任何普通的java业务系统程序代码当中的，是完全语法合格、能编译通过的，这是与传统CC工具的“代码残片”表示的动作代码所不同的地方</li>
<li>其中用到了上面介绍的“以正则语言来定义词法规则”、“以FluentInterfaces来定义语法规则”，以及“以闭包的形式来书写动作代码”（java中的闭包以“匿名内部类”的形式表示，道理一样）</li>
<li>其中，动作代码是一段段由<code>Action</code>接口实现的闭包（匿名内部类），里面的代码显然可以随意引用当前上下文中的所有变量、对象，这样实现了对上下文的capture，跟业务系统的无缝结合。</li>
<li><code>Action</code>闭包带有一个方法，参数是<code>Object... params</code>，这个参数里的值以位置顺序的方式对应于该action代码所附着的产生式里面的语法元素在parsing的时候所match到的值，听起来比较绕口，但实际上很简单，比如上面的 :</li>
</ul>


<p>带括号的表达式的action:</p>

<pre><code>.alt(LEFTPAREN, expr, RIGHTPAREN).action(new Action() {
    public Object act(Object... params) {
        return params[1];
    }
</code></pre>

<p>它的<code>params</code>的长度为3，分别是<code>["(", expr子规则匹配所返回的值, ")"]</code>，而这里我们的“业务(四则运算)”只需要关心<code>expr</code>的返回值，所以动作代码直接将<code>params[1]</code>（expr的返回值）返回给上一级匹配。<br/>
就是这样，实际上你可以在动作代码里面做任何事情，只要符合java语法的、你能想到的。</p>

<p>第一个版本的API设计基本就是这样了，往后可能还会加入一些语法糖，比如“Kleen闭包”来进一步方便语法规则的定义，不过会很慎重，因为dropincc.java是以小巧、易用为设计目标的，语法糖太多很可能会帮倒忙。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dropincc.java的概念与设计]]></title>
    <link href="http://pfmiles.github.com/blog/the-main-ideas-and-concepts-of-dropincc-dot-java"/>
    <updated>2012-03-17T17:40:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/the-main-ideas-and-concepts-of-dropincc-dot-java</id>
    <content type="html"><![CDATA[<h3>做这么个小lib的动机？</h3>

<p>其实由于java语言先天缺陷的元编程能力，在java的圈子里很少有人提到“DSL”的概念，就算有也是说说而已。与之相比，一些其它语言ruby, python, lisp等等，得益于其强大的元编程能力，直接利用自身语法特性，取一块语法子集，利用一些对象、类生命周期切入函数或者宏系统两三下就DSL &#8216;on-the-fly&#8217;了，这种便利性可能由java的设计目标来看，恐怕永远也无法达到了。<br/>
那么，在java里面想实现一个DSL，恐怕就得走“外部DSL(详见Fowler的文章: <a href="http://martinfowler.com/bliki/DomainSpecificLanguage.html">DomainSpecificLanguage</a>)”的路子。<br/>
而“外部DSL”这条路并不好走，说白了也就是自己做词法、语法、语义分析，自己生成目标语言代码去执行真正的逻辑；也就是做一个编译器前端外加一段解释执行逻辑；编译器前端，虽然业界看上去有很多很多工具可以直接采用，但是这些工具，目前看来，我认为还是太“General purpose”了，它们往往要求使用者学习它们自己规定的一套用作表达词法、语法规则的DSL，然后提供一个特殊的编译程序，用作编译你用它的DSL写的这些词法、语法文件，最终生成你想要的、基本不可读的编译器前端代码 —— 放到你的项目中去使用；<br/>
这种形式的工具在目前的编译器前端生成器中占大多数，使用上绝对称不上是“便利”。除了它用作描述规则的DSL需要使用者学习之外，其使用过程涉及好几个步骤与环境，是很麻烦的一件事情。<br/>
曾经有一次，一个同学试图使用antlr来做一个类似SQL子集的DSL，他花很多时间与antlr本身的语法文件、IDE “antlrworks”作斗争&#8230;频繁地在eclipse, 命令行, antlrWorks之间切换；我就想，做一个这样的DSL，是否真的用得上这么多工具？是否真的用得上号称能parse C++的antlr？这些事情能否全部在一个工作环境内搞定？比如eclipse？
而来自<a href="https://github.com/luikore/rsec">rsec</a>的启发让我觉得java似乎更应该有这样一个东西：</p>

<ol>
<li>以java的语法来新增词法、语法规则，不需要学习新的语法；</li>
<li>不需要依赖外部环境，仅在java开发环境中就能完成新语言的定义与编译器前端程序的生成；</li>
<li>虽然java是静态类型编译型语言，但整个新语言构建过程应该看上去是“动态的”、“解释性”的，这样“体验”才会好；</li>
<li>这个库要小、要没有其它三方库依赖，仅依靠JDK自带库就能使用；</li>
<li>在使用过程中尝试推行一些好的实践，确保大多数普通青年不会“自找麻烦”，比如“限制词法规则必须为正规文法(或者说乔姆斯基3型文法)”。</li>
</ol>


<p>而<a href="https://github.com/pfmiles/dropincc.java">Dropincc.java</a>正是以上述几点为设计目标的一个小巧(但足够强大)的编译器生成器。</p>

<!-- more -->


<h3>设计与实现思路</h3>

<p>以上第一点，基本上的想法就是，设计一套“串接与组合(cascading &amp; composition)”风格的API，模拟出一套parser combinator，让用户使用这套“貌似带有CC的语义”的java方法API来做词法、语法规则的添加(CC即：&#8221;compiler-compiler, 就是&#8217;编译器生成器&#8217;&#8221;)。<br/>
比如我想创建一套支持加减乘除的表达式计算器，还能使用括号调整运算优先级，我大概能用下面这样的形式直接在java代码中定义其词法、语法规则：</p>

<blockquote><p>注：下面的代码只是示意说明整体API风格，不一定是最终能运行的代码。</p></blockquote>

<pre><code>Lang lang = new Lang();
Token DIGIT = lang.addToken("\\d+");
Token ADD = lang.addToken("\\+");
Token SUB = lang.addToken("\\-");
Token MUL = lang.addToken("\\*");
Token DIV = lang.addToken("/");
Token LEFTPAREN = lang.addToken("\\(");
Token RIGHTPAREN = lang.addToken("\\)");

Grule expr = new Grule(); // grammar root
Grule term = new Grule();

Element mulTail = lang.addGrammarRule(MUL.or(DIV), term);
term.addGrammarRule(DIGIT, mulTail)
        .alt(LEFTPAREN, expr, RIGHTPAREN)
        .alt(DIGIT);
Element addendTail = lang.addGrammarRule(ADD.or(SUB), term);
expr.addGrammarRule(term , addendTail, CC.EOF);
</code></pre>

<p>其实想要表达的规则写成类似&#8221;BNF&#8221;的形式就是：</p>

<pre><code>// token rules
DIGIT ::= '\d+';
ADD ::= '+';
SUB ::= '-';
MUL ::= '*';
DIV ::= '/';
LEFTPAREN ::= '(';
RIGHTPAREN ::= ')';

// grammar rules
mulTail ::= (MUL | DIV) term;
term ::= DIGIT multail
       | LEFTPAREN expr RIGHTPAREN
       | DIGIT
       ;
addendTail ::= (ADD | SUB) term;
expr ::= term addendTail EOF;
</code></pre>

<p>这简直就是“line to line”的直译，而<a href="https://github.com/pfmiles/dropincc.java">Dropincc.java</a>的目标也就是要达到这样的效果；<br/>
这么做的目的并不是实现一套标准意义上的BNF；它使用了正则表达式来描述词法规则(token)，也就是上面提到的“目标”的第5点：“限制词法规则必须为正规文法”，这么做的意思也就是建议用户在“若词法规则中出现正规文法无法解决的意义冲突时”，将冲突“推后”到后面的语法、甚至语义分析阶段去解决；这种推荐的“较好实践”能够保持词法规则足够简单，使用正则表达式就能清晰地描述；如果用户不理解“保持词法规则足够简单”的重要性，只需要让他思考一下“为什么java的标识符不能由数字开头”应该就能想通了。<br/>
事实上就是：如果你不想自找麻烦，那么就应该保持词法规则足够简单 —— 你不应该在词法解析阶段耗费太多的实现精力 —— 这里用正则表达式就好了 —— 后面还有更麻烦、更有意义的活儿等着你去干呢&#8230;你不应该卡在词法分析这里太久&#8230;<br/>
好了，上面的这种约束一方面推行了一种“较好的实践”，另一方面也使得我们的这个工具更加直观、好用、简单，并且并不降低其实现语言的能力。</p>

<p>第二、三点，打算使用JDK1.6的新compiler API来实现；目前业界中的大多数CC工具都是“生成一个源文件”，让用户直接在项目中使用这个源文件；其实我认为这个东西以“源文件”的形式出现在项目中意义并不大，因为它几乎不可读(不可读的源代码有什么意义呢？)。只要有一个可执行的内存表示即可，“源文件”其实就不必了&#8230;这样也似乎使得项目代码看上去更“干净”。</p>

<p>至于第四点，也是很重要的，实现过程中一直保持“不依赖JDK以外的库”即可。</p>

<p>当所有的规则都定义完毕，只要在代码中触发&#8221;compile&#8221;，立刻就能基于定义的规则在内存中生成一个可执行的编译器前端，没有“源代码”，因为那没有意义，用户需要做的就只是拿它去执行新出生的语言的代码逻辑：</p>

<pre><code>lang.compile();
lang.exe("1+2+3*4/(5*6*7+8)");
</code></pre>

<p>当然，还有“action代码”的具体组织形式示例，上面没有给出，在<a href="dropincc-dot-java-api-design/">dropincc.java API Design</a>里面有详尽的阐述。<br/>
基本上对于这个工具的最终输出形式，计划中应该有两种：</p>

<ol>
<li>AST</li>
<li>直接执行的action</li>
</ol>


<p>输出AST其实是灵活度最大的一种形式，有了AST，那么后面是想解释执行？还是做翻译？那就交由用户去考虑了，dropincc.java的工作到这里也就结束了；<br/>
而“直接执行action”的方式适合像上面这种简单的诸如“四则运算”这样的简单表达式语言，在创建解析树的过程中就把对应的动作代码给执行了，解析结束后直接就能得到结果。</p>

<p>根据dropincc.java的后续设想，上述2种方式在dropincc.java中实现应该“几乎没有差别”，这两种输出形式虽然目的可能不一样，但在dropincc.java中实现起来应该是高度统一、一致的，这也是为了实现“足够简单”的目标，尽量不要让用户过多地去学习一些概念，增加上手成本。</p>

<p>至于dropincc.java能够用来识别哪种级别的语法？我的设想中dropincc.java是一个LL解析器生成器，最终要能识别LL(*)的语法，也就是跟antlr具有同等能力；不过初期的实现可能还识别不了那种程度的不确定性，这个可以一步一步随着版本推进慢慢来改进。其它一些诸如解决左递归的一些算法改进都可以排在计划中&#8230;</p>

<p>OK，前期的设想也就暂时介绍到这里，希望dropincc.java的努力能够将java圈子里的DSL应用甚至<a href="http://en.wikipedia.org/wiki/Language-oriented_programming">LOP</a>编程往前推进一步，这是相当令人振奋的事情。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Liquid Error about regexp match when using octopress-tagcloud]]></title>
    <link href="http://pfmiles.github.com/blog/liquid-error-about-regexp-match-when-using-octopress-tagcloud"/>
    <updated>2012-03-14T19:33:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/liquid-error-about-regexp-match-when-using-octopress-tagcloud</id>
    <content type="html"><![CDATA[<p>Octopress插件：<a href="https://github.com/tokkonopapa/octopress-tagcloud">octopress-tagcloud</a>相当nice，不过在使用的时候，若是blog整体采用了非ascii码的编码格式(比如utf-8)就会出现类似这样的错误：</p>

<pre><code>Liquid error: incompatible encoding regexp match (ascii-8bit regexp with utf-8 string)
</code></pre>

<p>其实是由于octopress-tagcloud的插件文件：<code>plugins/tag_cloud.rb</code>文件本身是ascii编码所致:</p>

<pre><code>$ chardet tag_cloud.rb
tag_cloud.rb: ascii (confidence: 1.00)
</code></pre>

<p><code>tag_cloud.rb</code>中很多地方用到了ruby的正则表达式，而ruby的正则表达式在匹配的时候，默认是按照“代码源文件”的编码格式(在这里是ascii)进行匹配的，而若我的blog是utf-8编码的话就会出现上述错误。</p>

<p>解决办法有二：</p>

<ol>
<li>将<code>tag_cloud.rb</code>转成utf-8&#8230;</li>
<li>更改<code>tag_cloud.rb</code>中所有的正则表达式声明，加上<code>u</code>选项(根据<a href="http://www.ruby-doc.org/core-1.9.3/Regexp.html">这里</a>的说明，<code>u</code>的意思就是以utf-8编码格式来进行匹配)，即，若原正则式是：<code>/regexp/</code>, 则改成：<code>/regexp/u</code></li>
</ol>


<!-- more -->


<p>这里是我改好的<code>tag_cloud.rb</code>：</p>

<div><script src='https://gist.github.com/2036006.js?file='></script>
<noscript><pre><code># Tag Cloud for Octopress, modified by pf_miles, for use with utf-8 encoded blogs(all regexp added 'u' option).
# =======================
# 
# Description:
# ------------
# Easy output tag cloud and category list.
# 
# Syntax:
# -------
#     {% tag_cloud [counter:true] %}
#     {% category_list [counter:true] %}
# 
# Example:
# --------
# In some template files, you can add the following markups.
# 
# ### source/_includes/custom/asides/tag_cloud.html ###
# 
#     &lt;section&gt;
#       &lt;h1&gt;Tag Cloud&lt;/h1&gt;
#         &lt;span id=&quot;tag-cloud&quot;&gt;{% tag_cloud %}&lt;/span&gt;
#     &lt;/section&gt;
# 
# ### source/_includes/custom/asides/category_list.html ###
# 
#     &lt;section&gt;
#       &lt;h1&gt;Categories&lt;/h1&gt;
#         &lt;ul id=&quot;category-list&quot;&gt;{% category_list counter:true %}&lt;/ul&gt;
#     &lt;/section&gt;
# 
# Notes:
# ------
# Be sure to insert above template files into `default_asides` array in `_config.yml`.
# And also you can define styles for 'tag-cloud' or 'category-list' in a `.scss` file.
# (ex: `sass/custom/_styles.scss`)
# 
# Licence:
# --------
# Distributed under the [MIT License][MIT].
# 
# [MIT]: http://www.opensource.org/licenses/mit-license.php
# 
module Jekyll

  class TagCloud &lt; Liquid::Tag

    def initialize(tag_name, markup, tokens)
      @opts = {}
      if markup.strip =~ /\s*counter:(\w+)/iu
        @opts['counter'] = $1
        markup = markup.strip.sub(/counter:\w+/iu,'')
      end
      super
    end

    def render(context)
      lists = {}
      max, min = 1, 1
      config = context.registers[:site].config
      category_dir = config['root'] + config['category_dir'] + '/'
      categories = context.registers[:site].categories
      categories.keys.sort_by{ |str| str.downcase }.each do |category|
        count = categories[category].count
        lists[category] = count
        max = count if count &gt; max
      end

      html = ''
      lists.each do | category, counter |
        url = category_dir + category.gsub(/_|\P{Word}/u, '-').gsub(/-{2,}/u, '-').downcase
        style = &quot;font-size: #{100 + (60 * Float(counter)/max)}%&quot;
        html &lt;&lt; &quot;&lt;a href='#{url}' style='#{style}'&gt;#{category.capitalize}&quot;
        if @opts['counter']
          html &lt;&lt; &quot;(#{categories[category].count})&quot;
        end
        html &lt;&lt; &quot;&lt;/a&gt; &quot;
      end
      html
    end
  end

  class CategoryList &lt; Liquid::Tag

    def initialize(tag_name, markup, tokens)
      @opts = {}
      if markup.strip =~ /\s*counter:(\w+)/iu
        @opts['counter'] = $1
        markup = markup.strip.sub(/counter:\w+/iu,'')
      end
      super
    end

    def render(context)
      html = &quot;&quot;
      config = context.registers[:site].config
      category_dir = config['root'] + config['category_dir'] + '/'
      categories = context.registers[:site].categories
      categories.keys.sort_by{ |str| str.downcase }.each do |category|
        url = category_dir + category.gsub(/_|\P{Word}/u, '-').gsub(/-{2,}/u, '-').downcase
        html &lt;&lt; &quot;&lt;li&gt;&lt;a href='#{url}'&gt;#{category.capitalize}&quot;
        if @opts['counter']
          html &lt;&lt; &quot; (#{categories[category].count})&quot;
        end
        html &lt;&lt; &quot;&lt;/a&gt;&lt;/li&gt;&quot;
      end
      html
    end
  end

end

Liquid::Template.register_tag('tag_cloud', Jekyll::TagCloud)
Liquid::Template.register_tag('category_list', Jekyll::CategoryList)</code></pre></noscript></div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[帮表妹和姑姑攒的台式机]]></title>
    <link href="http://pfmiles.github.com/blog/pc-assembled-for-my-cousin-and-aunt"/>
    <updated>2012-03-10T20:18:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/pc-assembled-for-my-cousin-and-aunt</id>
    <content type="html"><![CDATA[<p>其实早就看好的，不过一直没仔细上淘宝询问和调查；<br/>
现在在网上买电脑其实也还不错的；<br/>
她们平时也就拿电脑来偷偷菜，看看电影，玩玩小游戏逛逛taobao，这样的电脑跑到“电脑城”一问，人家跟他说要4k+才能拿下&#8230;我说这不坑爹么？行了，网上搞一套吧，拿回来自己装，心理预期价格2k左右 —— 反正不用买显示器。<br/>
对于这些“平民级”的PC需求，我好像一直对AMD体系情有独钟，包括我自己的台式也是AMD&#8230;话说我对台式机的需求还真是平民中的平民啊，反正我又不玩什么高端的游戏:)</p>

<h3>好了， 配置列表:</h3>

<ol>
<li>AMD a6-3650, 盒装带风扇，648元(小贵哦，不过看在它4核再加上显示核心的份上还行)</li>
<li>映泰a75 MH，全固态，小板子，主要是拿来配CPU的FM1插口，不管怎么说主板作为最重要的配件可要弄好一点，模拟/数字/HDMI接口都有(445元)</li>
<li>内存威刚 2G DDR3 1333 * 2 (152元, 2根用来组成双通道的，4g他们装个32位win7应该够了吧&#8230;)</li>
<li>硬盘WD500G WD5000AAKX 16M蓝盘 SATA3 (539元！好贵啊&#8230;最近硬盘水患涨价，不过总不能不要硬盘吧&#8230;)</li>
<li>电源航嘉冷钻2.3+, 300w (199元, 这玩意儿只能装进大机箱&#8230;)</li>
<li>华硕DVDRW DRW-24D1ST, 24x/SATA (135元，姑姑说她“想刻东西”&#8230;好吧&#8230;)<br/>
<strong><em>总：2118元</em></strong><br/>
木有键鼠，木有机箱（机箱在运输过程中怕容易坏所以没买），木有显示器（暂时用旧的），OK。哦，还有运费40&#8230;</li>
</ol>


<h3>总结</h3>

<p>久了不攒机真是out了啊out，殊不知现在流行显示核心嵌到CPU里面&#8230;而且也没想到硬盘由于工厂发大水涨价涨得厉害&#8230;哎<br/>
不过电脑这东西，平民级的用途的话，啥时候想用啥时候就买就行了，没有必要等，而且买了之后就没有什么好后悔的，因为这个行业的新产品出得太快了&#8230;当然也有人等着硬盘降价、等着显卡降价的，人家那不一样，人家那是发烧友，那是高玩，人家一个显卡就几千块的&#8230;能不等等么。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[面试中的算法]编程判断两个链表是否相交]]></title>
    <link href="http://pfmiles.github.com/blog/algorithms-in-job-interview-test-if-two-linked-lists-intersected"/>
    <updated>2012-03-06T23:30:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algorithms-in-job-interview-test-if-two-linked-lists-intersected</id>
    <content type="html"><![CDATA[<p>这是一道广为流传的题目: <a href="http://bop1.wikispaces.com/%E7%AC%AC%E4%B8%89%E7%AB%A0+%E7%BB%93%E6%9E%84%E4%B9%8B%E6%B3%95#x-%E7%AC%AC3%E7%AB%A0%20%E7%BB%93%E6%9E%84%E4%B9%8B%E6%B3%95%E2%80%94%E2%80%94%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8F%8A%E9%93%BE%E8%A1%A8%E7%9A%84%E6%8E%A2%E7%B4%A2-3.6%E7%BC%96%E7%A8%8B%E5%88%A4%E6%96%AD%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E6%98%AF%E5%90%A6%E7%9B%B8%E4%BA%A4">编程判断两个链表是否相交</a>;原题假设“不带环”，所以只要想通了之后很简单；但是，若要考虑带环的情况，那么要注意的点就很多了。
其实可以把无环和有环的情况全都包括在一个方法实现内解决。</p>

<h3>分析</h3>

<p>首先，无环的情况；无环是《编程之美》原书里的题目，很多人都反应说这个题相对书中其它题来讲太过于简单了。也确实，只要在纸上把“所有单向链表相交的情况”画出来很容易就能想通解法了（只要正确理解题意，那么“两个无环单向链表”画出来只可能是2条不相干的链表或一个&#8221;Y&#8221;字形） —— 所以，判断两个不带环的链表是否相交，只要将两个链表的头指针都移到链表尾，然后比较尾指针地址是否相等就可以了。
如果带环，个人总结，要明白以下几点：</p>

<ol>
<li>无环链表和有环链表是不可能相交的;</li>
<li>两个有环链表若相交，其“整个环上”的所有node一定都重合;</li>
<li>有环链表的相交，情况只有2种：相交于&#8221;环上&#8221;或相交于&#8221;不是环的部分&#8221;,即下图所示;两种情况都需要使用“两个指针的追逐”方法来判断两个链表的环部分是否相交;
<img src="http://pfmiles.github.com/images/circledLinkedListsIntersections.png" title="带环单向链表相交只有2种情况" alt="带环单向链表相交只有2种情况" /></li>
<li>有关链表追逐的考虑: 相对速度、距离、时间要算好，否则很容易漏掉几种边界情况;</li>
</ol>


<!-- more -->


<h3>代码</h3>

<pre><code>#include &lt;stdio.h&gt;

// define the node struct of links
typedef struct Node {
    struct Node* next;
} Node;

int is_intersected(Node* p1, Node* p2);

Node* has_circle(Node* head);

int main(int args, char** argv) {
    Node end1 = { NULL };
    Node end2 = { NULL };
    // 定义几种链表情况
    // two links not intersect with each other, no circle
    Node link_1_n =
            {
                    &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;end1}}}}}}}}};
    Node link_2_n =
            {
                    &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;end2}}}}}}}}};

    // two links intersect with each other, no circle
    Node common_n = { &amp;(Node) {&amp;(Node) {&amp;end1}}};

    Node link_1_y = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;common_n}}}}}};
    Node link_2_y = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;common_n}}}}}};

    // two links, has circle, not intersected.
    Node circle1 = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;circle1}}}}};
    Node link_c1_n = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;circle1}}}}};

    Node circle2 = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;circle2}}}}};
    Node link_c2_n = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;circle2}}}}};

    // two links, has circle, intersected at a non-circle position
    Node common_c = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;common_c}}}}};
    Node common_part = { &amp;(Node) {&amp;common_c}};

    Node link_c1_y = { &amp;(Node) {&amp;(Node) {&amp;common_part}}};
    Node link_c2_y = { &amp;(Node) {&amp;(Node) {&amp;common_part}}};
    // two links, has common circle, but different 'joint-points'.

    Node jp1 = { NULL };
    Node jp2 = { NULL };
    // 'weave' the joint-points into a circle:
    jp1.next = &amp;(Node) {&amp;(Node) {&amp;jp2}};
    jp2.next = &amp;(Node) {&amp;jp1};

    Node link_c1_y2 = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;jp1}}}}};
    Node link_c2_y2 = { &amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;(Node) {&amp;jp2}}}}};

    if (is_intersected(&amp;link_1_n, &amp;link_2_n)) {
        printf("link_1_n and link_2_n Intersected!\n");
    }

    if (is_intersected(&amp;link_1_y, &amp;link_2_y)) {
        printf("link_1_y and link_2_y Intersected!\n");
    }

    if (is_intersected(&amp;link_c1_n, &amp;link_c2_n)) {
        printf("link_c1_n and link_c2_n Intersected!\n");
    }

    if (is_intersected(&amp;link_c1_y, &amp;link_c2_y)) {
        printf("link_c1_y and link_c2_y Intersected!\n");
    }

    if (is_intersected(&amp;link_c1_y2, &amp;link_c2_y2)) {
        printf("link_c1_y2 and link_c2_y2 Intersected!\n");
    }
    return 0;
}

int is_intersected(Node* p1, Node* p2) {
    Node* has_circle_1 = has_circle(p1);
    Node* has_circle_2 = has_circle(p2);

    if (has_circle_1) {
        if (has_circle_2) {
            Node* pp1 = has_circle_1;
            Node* pp2 = has_circle_2;
            if (pp1 == pp2 || pp1-&gt;next == pp2)
                return 1;
            while (pp1-&gt;next != has_circle_1) {
                pp1 = pp1-&gt;next;
                pp2 = pp2-&gt;next-&gt;next;
                if (pp1 == pp2)
                    return 1;
            }
            return 0;
        } else {
            return 0;
        }
    } else {
        if (has_circle_2) {
            return 0;
        } else {
            while (p1-&gt;next)
                p1 = p1-&gt;next;
            while (p2-&gt;next)
                p2 = p2-&gt;next;
            return p1 == p2;
        }
    }
    return 0;
}

Node* has_circle(Node* head) {
    Node* p1;
    Node* p2;
    p1 = p2 = head;
    if (p2-&gt;next != NULL) {
        p2 = p2-&gt;next;
    } else {
        return NULL;
    }
    while (p2-&gt;next != NULL &amp;&amp; p2-&gt;next-&gt;next != NULL) {
        p1 = p1-&gt;next;
        p2 = p2-&gt;next-&gt;next;
        if (p1 == p2)
            return p1;
    }
    return NULL;
}
</code></pre>

<p>其中，<code>has_circle</code>方法是判断一个单向链表是否带环的，基本原理就是设置2个“速度”不同的链表，快的去追慢的，追上就是带环，直到较快指针遇到null还没追上就是没有环；假设环包含n个节点，指针<code>p2</code>的&#8221;速度&#8221;是2，<code>p1</code>的速度是1，相对速度就是1，从相同一点出发的话，<code>p2</code>追上<code>p1</code>至少要n步；若再假设该链表除了环的部分外还带有一个长度为k的“尾巴”，那么追上的步数最多是n+k;也就是线性时间复杂度内就能完成这个判断。</p>

<p>这提供了一种很好的判断是否&#8221;环状&#8221;的思路；以前我只写过“用一个栈来记录”的方式，弱爆了&#8230;(时间复杂度为O(n<sup>2</sup>))</p>

<p>在<code>has_circle_1</code>和<code>has_circle_2</code>都满足的时候，也就是说2个链表都带环的时候，要分别取2个环上的一点来玩“追逐游戏”来判断是否相交；在这段程序里是<code>pp1</code>和<code>pp2</code>;然后一个速度为2一个速度为1开始玩“追逐游戏”，当慢的那个走完环上所有节点时快的那个还没追上它的话，说明不相交(此时耗费时间n——即环节点数;因为快慢指针的相对速度为1，快指针理应在时间n以内追上慢链表，否则不相交)。</p>

<h3>总结</h3>

<p>单向链表的问题&#8230;着实不简单，可以相当复杂&#8230;对于这种关乎“形状”的问题，在纸上画一画会很有帮助。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[编程之美]寻找最大的k个数]]></title>
    <link href="http://pfmiles.github.com/blog/find-the-max-k-numbers"/>
    <updated>2012-03-02T13:37:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/find-the-max-k-numbers</id>
    <content type="html"><![CDATA[<p>《编程之美》上的一道题： <a href="http://bop1.wikispaces.com/%E7%AC%AC%E4%BA%8C%E7%AB%A0+%E6%95%B0%E5%AD%97%E4%B9%8B%E9%AD%85#x-2.5%20%E5%AF%BB%E6%89%BE%E6%9C%80%E5%A4%A7%E7%9A%84K%E4%B8%AA%E6%95%B0%20%E2%98%85%E2%98%85%E2%98%85">wiki链接</a>;
其实在<a href="http://blog.csdn.net/v_JULY_v/article/details/6370650">July的文章里有更加深刻的分析</a>，我这里只是用最小堆的方式实现一遍，体会和总结一下。</p>

<h3>分析</h3>

<ul>
<li>这个问题，只要找到“最大的K个数”，但并不要求这K个数之间排序；</li>
<li>而且如果K很小的话，完全可以采用选择排序的方式，达到O(nk)的复杂度；</li>
<li>若K不小，则可利用这“只能容纳K个数的最小堆”的方法来做；时间复杂度是O(nlogk);用“堆”这种结构来解决此问题，很好地理解和利用了堆的特性:

<ol>
<li> 找最大的K个数要使用最小堆，是因为最小堆总能最方便地剔除“最小的元素”，那么最后剩在堆中的都是最大的K个元素了；反之，若要找“最小的K个元素”则应该使用最大堆</li>
<li> 二叉堆的同层节点之间是没有顺序的，这恰好符合我们的题目“不要求K个数之间排序”的需求，少了K个数之间的排序则能够减少不必要计算</li>
</ol>
</li>
</ul>


<h3>程序</h3>

<pre><code>#include &lt;stdio.h&gt;

// defines the heap structure
typedef struct {
    int size, last_pos;
    int* data;
} Heap;

void into_min_heap(int node, Heap* heap);
void adjust_min_heap(Heap* heap, int cur_index);

// find the max k digits
void max_k(int len, int array[], int k, int rst[]) {
    // build min-heap on rst while iterating array:
    // worst time complexity: n * log k
    int i;
    Heap heap = { k, -1, rst };
    for (i = 0; i &lt; len; i++) {
        into_min_heap(array[i], &amp;heap);
    }
}

void into_min_heap(int node, Heap* heap) {
    if (heap-&gt;last_pos == -1) {
        heap-&gt;data[0] = node;
        heap-&gt;last_pos = 0;
    } else {
        if (node &lt; heap-&gt;data[0]) {
            return;
        } else {
            if (heap-&gt;last_pos + 1 &lt; heap-&gt;size) {
                heap-&gt;data[heap-&gt;last_pos + 1] = node;
                heap-&gt;last_pos++;
            } else {
                heap-&gt;data[0] = node;
                adjust_min_heap(heap, 0); // log k
            }
        }
    }
}

void adjust_min_heap(Heap* heap, int cur_index) {
    int c1_index = (cur_index + 1) * 2;
    int c2_index = (cur_index + 1) * 2 + 1;
    if (c1_index &lt; heap-&gt;size) {
        if (heap-&gt;data[cur_index] &lt;= heap-&gt;data[c1_index]) {
            if (c2_index &lt; heap-&gt;size) {
                if (heap-&gt;data[cur_index] &lt;= heap-&gt;data[c2_index]) {
                    return;
                } else {
                    int tmp = heap-&gt;data[cur_index];
                    heap-&gt;data[cur_index] = heap-&gt;data[c2_index];
                    heap-&gt;data[c2_index] = tmp;
                    adjust_min_heap(heap, c2_index);
                }
            }
        } else {
            int tmp = heap-&gt;data[cur_index];
            heap-&gt;data[cur_index] = heap-&gt;data[c1_index];
            heap-&gt;data[c1_index] = tmp;
            adjust_min_heap(heap, c1_index);
        }
    }
}

int main(int args, char** argv) {
    int k = 3;
    int rst[k];
    int arr[10] = { 1, 20, -35, 4, 7, 11, 19, -5, 0, 18 };
    max_k(10, arr, k, rst);
    printf("The max k numbers are: ");
    int i;
    for (i = 0; i &lt; k; i++) {
        printf("%d ", rst[i]);
    }
    return 0;
}
</code></pre>

<h3>总结</h3>

<p>还是要理解各种常用数据结构的特性及其意义，遇到特定的问题时才好能够正确地选择出最能适合需求的这种结构。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[跟踪递归函数路径的方法]]></title>
    <link href="http://pfmiles.github.com/blog/trace-the-path-of-recursive-function"/>
    <updated>2012-02-29T00:04:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/trace-the-path-of-recursive-function</id>
    <content type="html"><![CDATA[<p>有些时候，需要追踪递归函数的路径；比如：递归地遍历一棵树，当找到符合要求的节点时，打印出从根节点到此节点的路径。</p>

<p>这种时候，若子节点不持有到父节点的指针，则需要对路径进行随时地记录，以便需要时能够获取；</p>

<ul>
<li>一种方式就是建立“栈”结构，在每次递归时将当前节点压栈，并且记得在递归调用过后立刻<strong><em>出栈</em></strong>；</li>
<li>另一种方式就是直接利用函数递归调用的“调用栈”(也就是传参)了。</li>
</ul>


<p><em>例如有一个题目：输入一个整数和一棵二元树。
从树的根结点开始往下访问一直到叶结点所经过的所有结点形成一条路径。
打印出和与输入整数相等的所有路径。</em></p>

<p>程序较简单，就是递归遍历，查找节点相加数值等于预期数值的情况：</p>

<pre><code># define the data-structure first
class Node(object):
    def __init__(self, left=None, value=None, right=None):
    self.value = value
    self.left = left
    self.right = right

root = Node(Node(Node(None, 4, None), 5, None), 10 , Node(None, 12, Node(None, 7, None)))

found = []
def findPath(tree, value, sm=0, paths=[]):
    if tree.value + sm &gt; value:
    return
    elif tree.value + sm == value:
    paths.append(tree.value)
    found.append(paths[:])
    return
    else:
    paths.append(tree.value)
    if tree.left != None:
        findPath(tree.left, value, tree.value + sm, paths)
    if tree.right !=None:
        findPath(tree.right, value, tree.value + sm, paths)
    paths.pop()

findPath(root, 22)
print found
</code></pre>

<p>其中值得注意的是：</p>

<pre><code>paths.append(tree.value)
</code></pre>

<p>和：</p>

<pre><code>paths.pop()
</code></pre>

<p>这两行；
在递归方法前后进行对应地push和pop是正确记录路径必不可少的步骤。</p>

<p>其实，像上述这种每次递归调用之前都push，每次调用之后都pop的情况，其栈的层次、深度、变化规律跟函数调用栈是完全一致的，那么其实就完全可以考虑直接把父节点扔给下一层递归函数：</p>

<pre><code>findPath(tree.left, value, tree.value + sm, tree)
</code></pre>

<p>其实，findPath的函数签名现在就变成了：</p>

<pre><code>findPath(树, 预期值, 当前和, 父节点)
</code></pre>

<p>这样一来，其实路径上的每一步，都是记录在调用栈上的(就算子节点没有父节点的指针，函数参数也直接把父节点带过来了，也就是在调用栈中)，当程序想要输出路径时，也完全能够将路径还原。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[面试中的算法]求子数组的最大和]]></title>
    <link href="http://pfmiles.github.com/blog/algorithms-in-job-interview-max-sum-sub-list"/>
    <updated>2012-02-28T22:48:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algorithms-in-job-interview-max-sum-sub-list</id>
    <content type="html"><![CDATA[<p>此问题由<a href="http://blog.csdn.net/v_JULY_v/article/details/6050133">v_JULY_v</a>整理发布并发表于<a href="http://blog.csdn.net/v_JULY_v">blog</a>上, 版权归原作者所有。</p>

<p><strong>输入一个整形数组,数组里有正数也有负数。
数组中连续的一个或多个整数组成一个子数组,每个子数组都有一个和。
求所有子数组的和的最大值。要求时间复杂度为 O(n)。
例如输入的数组为 1, -2, 3, 10, -4, 7, 2, -5,和最大的子数组为 3, 10, -4, 7, 2, 因此输出为该子数组的和 18</strong></p>

<h3>分析：</h3>

<p>贪心算法能在此问题的任意一个子问题上找到最优解(最优子结构)，所以可以用贪心算法解决。</p>

<!-- more -->


<h3>程序：</h3>

<pre><code># 1, -2, 3, 10, -4, 7, 2, -5, output the maximum summarized sub-list
# greedy algorithm
def greatestSumSubList(al):
    mx = 0
    sm = 0
    for i in al:
    sm += i
    if sm &lt; 0:
        sm = 0
    if sm &gt; mx:
        mx = sm
    return mx

print greatestSumSubList([1, -2, 3, 10, -4, 7, 2, -5])
</code></pre>

<p>输出： 18</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[面试中的算法]设计包含min函数的栈]]></title>
    <link href="http://pfmiles.github.com/blog/algorithms-in-job-interview-design-a-stack-with-min-function"/>
    <updated>2012-02-28T00:14:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algorithms-in-job-interview-design-a-stack-with-min-function</id>
    <content type="html"><![CDATA[<p>此问题由<a href="http://blog.csdn.net/v_JULY_v/article/details/6050133">v_JULY_v</a>整理发布并发表于<a href="http://blog.csdn.net/v_JULY_v">blog</a>上, 版权归原作者所有。</p>

<p><strong>问题描述：定义栈的数据结构,要求添加一个 min 函数,能够得到栈的最小元素, 要求函数 min、push 以及 pop 的时间复杂度都是 O(1)</strong></p>

<h3>分析：</h3>

<p>主要是抓住栈的<strong>“还原现场”</strong>的能力 —— 由于随着元素的加入与弹出，“最小元素”随时可能变化；在栈的每一个状态，都要维护一个当前最小元素的记录;随着push或pop，“最小元素”也跟着更新或还原到上一次的状态；所以，“最小元素”应该随着栈元素被记录在栈的每一层。</p>

<!--more-->


<h3>程序：</h3>

<pre><code>class MinStack(object):
    def __init__(self):
    self.arr = []

    def push(self, ele):
    if len(self.arr) == 0:
        # no elements yet
        self.arr.append((ele, ele));# pushes (ele, minEle)
    else:
        self.arr.append((ele, ele if ele &lt; self.arr[-1][1] else self.arr[-1][1]))

    def pop(self):
    return self.arr.pop()[0]

    # using name 'myMin' instead of 'min' because 'min' is a built-in method
    def myMin(self):
    return self.arr[-1][1]

stack = MinStack()
stack.push(5)
print stack.myMin() # 5 expected
stack.push(-1)
stack.push(10)
stack.push(9)

print stack.myMin() # -1 expected
print stack.pop() # 9 expected
</code></pre>

<p>要点：利用栈的“还原上一次记录”的能力, 随时维护当前最小元素</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[面试中的算法]把二元查找树转变成排序的双向链表]]></title>
    <link href="http://pfmiles.github.com/blog/algorithms-in-job-interview-turn-binary-search-tree-to-doubly-linked-list"/>
    <updated>2012-02-27T23:26:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/algorithms-in-job-interview-turn-binary-search-tree-to-doubly-linked-list</id>
    <content type="html"><![CDATA[<p>此问题由<a href="http://blog.csdn.net/v_JULY_v/article/details/6050133">v_JULY_v</a>整理发布并发表于<a href="http://blog.csdn.net/v_JULY_v">blog</a>上, 版权归原作者所有。</p>

<p><strong>问题描述：输入一棵二元查找树,将该二元查找树转换成一个排序的双向链表。要求不能创建任何新的结点,只调整指针的指向</strong></p>

<p>比如二叉搜索树：</p>

<pre><code>       10
      /   \
    4      12
   / \    /  \
  3   5 11    13
</code></pre>

<p>输出应为： 3, 4, 5, 10, 11, 12, 13</p>

<h3>分析：</h3>

<p>处理树状结构很容易想到递归，而二叉搜索树其实恰好是已经排序好的一个结构，而要把它变成数组，只需“中序遍历”即可: 3,4,5,10,11,12,13</p>

<!--more-->


<h3>程序：</h3>

<pre><code># define the data-structure first
class Node(object):
    def __init__(self, left=None, value=None, right=None):
        self.value = value
        self.left = left
        self.right = right

# construct the bin-search tree 
root = Node(None, 10, None)

left = Node(Node(None, 3, None), 4, Node(None, 5, None))

right = Node(Node(None, 11, None), 12, Node(None, 13, None))

root.left = left
root.right = right

def show(node):
    rst = [node.value]
    leftCur, rightCur = node, node
    while leftCur.left != None:
        rst.insert(0, leftCur.left.value)
        leftCur = leftCur.left
    while rightCur.right != None:
        rst.append(rightCur.right.value)
        rightCur = rightCur.right
    print rst

# 3,4,5,10,11,12,13 expected

# the recursive mid-order traverse function
def traverse(node, leftOrRight = None):
    l, r = None, None
    if node.left != None:
        l = traverse(node.left, "left")
        node.left = l
        l.right = node
    if node.right != None:
        r = traverse(node.right, "right")
        node.right = r
        r.left = node
    if leftOrRight == "left" and r != None:
        return r
    elif leftOrRight == "right" and l != None:
        return l
    else:
        return node

n = traverse(root)

show(n)
</code></pre>

<p>结果输出： [3, 4, 5, 10, 11, 12, 13]</p>

<p>要点除了搞清楚二叉搜索树的特性以及中序遍历的规律外，要注意traverse方法中对于该返回&#8217;l&#8217; 还是 &#8216;r&#8217; 或者是node的判断</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[翻译]Python性能诀窍]]></title>
    <link href="http://pfmiles.github.com/blog/python-speed-performance-tips"/>
    <updated>2012-02-05T18:45:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/python-speed-performance-tips</id>
    <content type="html"><![CDATA[<p><strong>以下文字译自<a href="http://wiki.python.org/moin/PythonSpeed/PerformanceTips">http://wiki.python.org/moin/PythonSpeed/PerformanceTips</a></strong></p>

<p>这个页面提供了一些帮助改善你的python程序性能的一些点子或小技巧。<br/>
自从我在大约1996年编写了第一个&#8221;fast python&#8221;页面以来，Python在许多关键的方面有了改变，这意味着一部分内容必须要调整。我将其整合到Python wiki中从而希望其他人能帮助维护它。</p>

<blockquote><p>你应该始终针对你的应用和你打算使用的、特定的<a href="http://wiki.python.org/moin/PythonImplementations">python实现</a>来尝试这些小技巧而不是盲目地相信一种方法比另一种更高效。更多的细节请参考<a href="#Profiling">profiling</a>一节。</p></blockquote>

<p>另外自从这个页面最初编写以来，新出现的包例如<a href="http://cython.org/">Cython</a>, <a href="http://www.cosc.canterbury.ac.nz/~greg/python/Pyrex/">Pyrex</a>, <a href="http://psyco.sourceforge.net/">Psyco</a>, <a href="http://www.scipy.org/Weave">Weave</a>和<a href="http://pyinline.sourceforge.net/">PyInline</a>, 都能通过更容易地将性能要求较高的代码推到C或机器码上去实现从而大幅地改进你的程序的性能。</p>

<h3>概览：优化那些需要优化的地方</h3>

<p>你只能在程序首先给出正确的结果之后才能知道哪里致使你的程序慢了，所以先保证程序正确，再跑程序看是否慢。
当发现慢了，性能监控能发现程序的哪部份消耗了大部分的时间。一套全面、方便运行的测试能保证后面的优化没有改变你的程序的正确性。<br/>
简而言之：</p>

<ol>
<li>把程序调正确。</li>
<li>测试程序的正确性。</li>
<li>如果发现慢，分析它的性能。</li>
<li>优化。</li>
<li>从第&#8217;2&#8217;步开始重复。</li>
</ol>


<p>某些优化方法刚好就是一个好的编程风格，所以你应该在学习这门语言的时候就学习它们。其中一个例子就是移动那些在循环内或循环外都不会改变的值计算。</p>

<!--more-->


<h3>选择正确的数据结构</h3>

<p>TBD.</p>

<h3>排序</h3>

<p>对list中的基本python对象进行排序一般来讲还是很高效的。list的sort方法接受一个可选的比较方法用来控制排序行为。虽然由于这比较方法会被调用多次，这会显著地减慢你的排序，但是这很方便。在Python 2.4中，你应该使用内置sort函数的&#8217;key&#8217;参数，这是实现排序的最便捷的方式。</p>

<p>仅当你在使用老版本的Python(2.4以前)的时候需要遵照&#8217;Guido van Rossum&#8217;的建议：</p>

<p>加速排序的一个变通的方式就是构造一个tuple的列表，每个tuple的第一个元素是一个会被默认的排序方式正确排序的排序key，tuple的第二个元素是原本的被排序对象。这种方法被称为“Schwartzian变换”，也被称为“Decorate SortUndecorate (DSU)”。</p>

<p>假设，例如，你拥有一个list的tuple，你想依据每个tuple的第n个元素对整个list进行排序。下面的方法能够实现。</p>

<pre><code>def sortby(somelist, n):
    nlist = [(x[n], x) for x in somelist]
    nlist.sort()
    return [val for (key, val) in nlist]
</code></pre>

<p>改变当前list进行排序也很容易被实现：</p>

<pre><code>def sortby_inplace(somelist, n):
    somelist[:] = [(x[n], x) for x in somelist]
    somelist.sort()
    somelist[:] = [val for (key, val) in somelist]
    return
</code></pre>

<p>这里有一个例子：</p>

<pre><code>&gt;&gt;&gt; somelist = [(1, 2, 'def'), (2, -4, 'ghi'), (3, 6, 'abc')]
&gt;&gt;&gt; somelist.sort()
&gt;&gt;&gt; somelist
[(1, 2, 'def'), (2, -4, 'ghi'), (3, 6, 'abc')]
&gt;&gt;&gt; nlist = sortby(somelist, 2)
&gt;&gt;&gt; sortby_inplace(somelist, 2)
&gt;&gt;&gt; nlist == somelist
True
&gt;&gt;&gt; nlist = sortby(somelist, 1)
&gt;&gt;&gt; sortby_inplace(somelist, 1)
&gt;&gt;&gt; nlist == somelist
True
</code></pre>

<p>来自Tim Delaney</p>

<p>从Python2.3开始sort都是稳定的。(译者注：即相等元素不会被随意调整先后次序)</p>

<p>(确切地说, sort在CPython2.3中是稳定的, 在Python 2.4中被保证是稳定的)</p>

<p>Python2.4新增了一个可选的key参数使得这个变换容易多了：(译者注：key参数要求传入一个函数)</p>

<pre><code># E.g. n = 1
n = 1
import operator
nlist.sort(key=operator.itemgetter(n))
# use sorted() if you don't want to sort in-place:
# sortedlist = sorted(nlist, key=operator.itemgetter(n))
</code></pre>

<p>请注意原本的元素并没有被用作排序，用作排序的仅仅是返回的key —— 这相当于下面这样做：</p>

<pre><code># E.g. n = 1
n = 1
nlist = [(x[n], i, x) for (i, x) in enumerate(nlist)]
nlist.sort()
nlist = [val for (key, index, val) in nlist]
</code></pre>

<h3>字符串拼接：</h3>

<blockquote><p>在新一些的版本的python中，这一小节的内容的准确性存在争议。在CPython 2.5中，字符串拼接已经相当地快，但并不能说其它python实现也是这样。具体请参考<a href="http://wiki.python.org/moin/ConcatenationTestCode">ConcatenationTestCode</a>中的讨论。</p></blockquote>

<p>字符串在python中是不可变的。这一潜在的事实经常咬住那些python初学者的屁股。不可变的特性带来一些优点和缺点。优点就是，字符串可以被用作dict的key，并且可以在多个变量间安全地共享同一个实例。(Python自动共享只含1个或2个字符的字符串。)缺点就是，在任何字符串中你不能办到这样的事情：“将所有的&#8217;a&#8217;换成&#8217;b&#8217;”。取而代之的实现方式是你必须重新新建一个字符串来达到目的。这种连续不断的字符串拷贝给python程序带来了不可忽视的性能问题。</p>

<p>避免这样做：</p>

<pre><code>s = ""
for substring in list:
    s += substring
</code></pre>

<p>Use s = &#8220;&#8221;.join(list) instead. The former is a very common and catastrophic mistake when building large strings. Similarly, if you are generating bits of a string sequentially instead of:
应该用s = &#8220;&#8221;.join(list)来实现。上面的代码是一种在构造大字符串时很常见的、灾难性的错误。相似地，当你处理字符串中的每一个字符时，不要使用：</p>

<pre><code>s = ""
for x in list:
    s += some_function(x)
</code></pre>

<p>而是用：</p>

<pre><code>slist = [some_function(elt) for elt in somelist]
s = "".join(slist)
</code></pre>

<p>还有，避免这样做：</p>

<pre><code>out = "&lt;html&gt;" + head + prologue + query + tail + "&lt;/html&gt;"
</code></pre>

<p>取而代之要这样做：</p>

<pre><code>out = "&lt;html&gt;%s%s%s%s&lt;/html&gt;" % (head, prologue, query, tail)
</code></pre>

<p>更好的做法，为了可读性(效率没有进一步提升，只是可读性更好)，使用字典替换：</p>

<pre><code>out = "&lt;html&gt;%(head)s%(prologue)s%(query)s%(tail)s&lt;/html&gt;" % locals()
</code></pre>

<p>后两段代码会快很多，特别是在CGI脚本扩展运行很多次的时候，且更容易被更改、启动。而且，较慢的方式在python2.0中会变得更慢。它使得python虚拟机花更多的时间搞清楚如何拼接2个字符串。(别忘了python是在运行时查找所有的方法)</p>

<h3>循环</h3>

<p>Python支持好几种循环结构。for语句是最常用的。它遍历一个序列的每个元素，将每个元素赋值给循环变量。如果你的循环体很简单，for循环本身的解释成本将占据大部分的开销。这个时候<a href="http://www.python.org/doc/lib/built-in-funcs.html">map</a>函数就能派上用场了。你可以将map函数看作是for循环采用C代码来实现。唯一的约束是“循环体”必须是一个函数调用。list comprehension除了语法上的便利性之外，他们常常和等价的map调用一样快甚至更快。</p>

<p>这里有一个很直接的例子。取代循环遍历的方式将列表中所有单词变成大写：</p>

<pre><code>newlist = []
for word in oldlist:
    newlist.append(word.upper())
</code></pre>

<p>你可以使用map函数将这个循环由解释执行推到编译好的C代码中去执行：</p>

<pre><code>newlist = map(str.upper, oldlist)
</code></pre>

<p>List comprehension在python 2.0的时候被加入。它们提供了一种更紧凑的语法和更高效的方式来表达上面的for循环：</p>

<pre><code>newlist = [s.upper() for s in oldlist]
</code></pre>

<p>generator表达式在python 2.4中被加入。它们功能上跟list comprehension或者map函数差不多不过避免了立刻生成整个列表的开销。相应的，它们返回一个generator对象，能够一个一个元素地被遍历：</p>

<pre><code>iterator = (s.upper() for s in oldlist)
</code></pre>

<p>哪种方式更合适取决于你使用的python版本以及你处理的数据的特性。</p>

<p>Guido van Rossum写过一份更详细的(很简洁的)关于<a href="http://www.python.org/doc/essays/list2str.html">循环优化的分析</a>很值得一读。</p>

<h3>避免过多的&#8217;.&#8217;</h3>

<p>假如你不能使用map函数或list comprehension呢？你很可能就被这for循环卡住了。for循环还有另外一个坏处。newList.append和word.upper都是函数引用并且在每次循环都被解释执行。原循环可被变成这样：</p>

<pre><code>upper = str.upper
newlist = []
append = newlist.append
for word in oldlist:
    append(upper(word))
</code></pre>

<p>这个技巧使用时应谨慎。如果循环体很大，代码将会很难维护。除非你非常熟悉这块代码并且很容易找到append和upper的定义。</p>

<h3>局部变量</h3>

<p>对于非map版本的循环，最后一种优化手段就是尽可能地使用局部变量。如果上述循环被封装成一个函数，append和upper就成了局部变量。Python访问局部变量比全局变量高效得多。</p>

<pre><code>def func():
    upper = str.upper
    newlist = []
    append = newlist.append
    for word in oldlist:
        append(upper(word))
    return newlist
</code></pre>

<p>当我写这篇文章时我正在使用100MHz的奔腾处理器，跑着BSDI系统。将/usr/share/dict/words中的单词列表转换成大写，时间开销如下：</p>

<pre><code>Version Time (seconds)
Basic loop 3.47
Eliminate dots 2.45
Local variable &amp; no dots 1.79
Using map function 0.54
</code></pre>

<h3>初始化字典元素</h3>

<p>假设你正在使用dict来记录单词出现频率，且你已经将要处理的文本整理成了单词列表。很可能你会像下面这样做：</p>

<pre><code>wdict = {}
for word in words:
    if word not in wdict:
        wdict[word] = 0
    wdict[word] += 1
</code></pre>

<p>除了第一次执行以外，每次一个新词被发现后if条件就会是false。如果你处理很大一个单词集合，很多单词很可能出现多次。像这种一个值只初始化一次但是会被更改多次的情况，使用try语句会划算一些：</p>

<pre><code>wdict = {}
for word in words:
    try:
        wdict[word] += 1
    except KeyError:
        wdict[word] = 1
</code></pre>

<p>关键是要catch <a href="http://wiki.python.org/moin/KeyError">KeyError</a>这个异常，且不包含一个默认的except语句来试图从任何未预期的错误中恢复。</p>

<p>在python 2.x发布后还有第三种选择。dict拥有了一个get()方法，当找不到指定的key的时候会返回一个默认值。这能够简化循环：</p>

<pre><code>wdict = {}
get = wdict.get
for word in words:
    wdict[word] = get(word, 0) + 1
</code></pre>

<p>当我最初写这一小节的时候，有一些显然的场景使得前两种方式更快。不过现在貌似3种方式的性能是差不多的(互相之间差距在10%以内)，跟列表的属性或里面的单词无关。</p>

<p>而且，如果存在dict里面的东西是object或一些可变的列表对象，你也可以使用dict.setdefault方法，比如：</p>

<pre><code>wdict.setdefault(key, []).append(new_element)
</code></pre>

<p>你可能想这样做避免了两次查找key。实际上不是的(就算是在python 3.0里面)，不过至少这2次查找是在C代码中完成的。</p>

<p>另一种选择是使用<a href="http://docs.python.org/py3k/library/collections.html#collections.defaultdict">defaultdict</a>类：</p>

<pre><code>from collections import defaultdict

wdict = defaultdict(int)

for word in words:
    wdict[word] += 1
</code></pre>

<h3>import语句的开销</h3>

<p>import语句能够在任何地方被执行。它们常常被放在函数内部以便于限制它们的作用范围且/或节省启动初始化时间。虽然python的解释器被优化得不会重复import同一个模块，重复地执行import语句在一些情况下仍然能显著地影响性能。</p>

<p>考虑下面这两块代码(出自<a href="http://wiki.python.org/moin/McFarlane">Greg McFarlane</a>，我相信——我发现这代码在comp.lang.python python-list@python.org帖子里没有被署名成他但后来我在另外一个地方看到这块代码被署名成他)</p>

<pre><code>def doit1():
    import string ###### import statement inside function
    string.lower('Python')

for num in range(100000):
    doit1()
</code></pre>

<p>或者：</p>

<pre><code>import string ###### import statement outside function
def doit2():
    string.lower('Python')

for num in range(100000):
    doit2()
</code></pre>

<p>虽然string模块的引用在doit2中是全局的，doit2仍会比doit1跑得快得多。这里有一段比较程序使用python2.3和最新的timeit模块，来看看第二个到底比第一个快多少：</p>

<pre><code>&gt;&gt;&gt; def doit1():
... import string
... string.lower('Python')
...
&gt;&gt;&gt; import string
&gt;&gt;&gt; def doit2():
... string.lower('Python')
...
&gt;&gt;&gt; import timeit
&gt;&gt;&gt; t = timeit.Timer(setup='from __main__ import doit1', stmt='doit1()')
&gt;&gt;&gt; t.timeit()
11.479144930839539
&gt;&gt;&gt; t = timeit.Timer(setup='from __main__ import doit2', stmt='doit2()')
&gt;&gt;&gt; t.timeit()
4.6661689281463623
</code></pre>

<p>String相关方法在python2.0时被引入。这带来了一种完全避免import语句的方式并且运行得更加快：</p>

<pre><code>def doit3():
    'Python'.lower()

for num in range(100000):
    doit3()
</code></pre>

<p>这是来自timeit的证明：</p>

<pre><code>&gt;&gt;&gt; def doit3():
... 'Python'.lower()
...
&gt;&gt;&gt; t = timeit.Timer(setup='from __main__ import doit3', stmt='doit3()')
&gt;&gt;&gt; t.timeit()
2.5606080293655396
</code></pre>

<p>上面这例子显然有点做作，但内在的道理是正确的。</p>

<p>注意，将import放在函数内部能加速模块的初始化加载，特别是在被引用的模块不一定会被用到的时候。这通常就是一个“懒”加载的情形 —— 在你真正需要它(引入一个模块是开销很大的)之前避免引入它。</p>

<p>这种好处仅仅出现在这个被引入模块还未被加载之前（在任何地方） —— 如果模块已经被加载了(许多标准库常常会出现这种情况，比如string或re)，这时再避免import它不会给你带来任何节省。可以通过sys.modules查看哪些模块被引入了。</p>

<p>做到懒加载的一种较好的方式是：</p>

<pre><code>email = None

def parse_email():
    global email
    if email is None:
        import email
    ...
</code></pre>

<p>这样一来email模块仅会被引入一次，在parse_email()被第一次调用的时候。</p>

<h3>数据集合</h3>

<p>函数调用的成本在python中相对较高，特别是跟那些内置函数的执行速度比起来。强烈建议在所有可能的时候，函数应该处理数据集合。这里有一个故意写出的例子：</p>

<pre><code>import time
x = 0
def doit1(i):
    global x
    x = x + i

list = range(100000)
t = time.time()
for i in list:
    doit1(i)

print "%.3f" % (time.time()-t)
</code></pre>

<p>vs.</p>

<pre><code>import time
x = 0
def doit2(list):
    global x
    for i in list:
        x = x + i

list = range(100000)
t = time.time()
doit2(list)
print "%.3f" % (time.time()-t)
</code></pre>

<p>这里有一个亲自体验的交互式的例子：</p>

<pre><code>&gt;&gt;&gt; t = time.time()
&gt;&gt;&gt; for i in list:
... doit1(i)
...
&gt;&gt;&gt; print "%.3f" % (time.time()-t)
0.758
&gt;&gt;&gt; t = time.time()
&gt;&gt;&gt; doit2(list)
&gt;&gt;&gt; print "%.3f" % (time.time()-t)
0.204
</code></pre>

<p>就算使用python写的，第二个例子也比第一个例子运行得快约4倍。若doit是用C写的，这区别应该会更大（将python的for loop换成C的for loop同时也去掉了大多数的函数调用）。</p>

<h3>做更少的后台动作</h3>

<p>Python的解释器会做一些周期性的检查。比如，它要确定是否让另一个线程开始工作或者是否应该执行一个等待中的调用(一般来说是一个由信号处理器建立的调用)。由于大多数时候实际上不需要做这些事情，所以在每个解释器周期都执行这些检查会让事情变慢。在sys模块中有一个函数，setcheckinterval，可以让你设置解释器执行这些周期性检查的频率。在python2.3之前默认值是10。2.3之后提高为100。如果你没有执行多线程操作且你不需要接收许多信号量，把它设置成一个更大些的值可以提升解释器的性能，有时候还很显著。</p>

<h3>Python不是C</h3>

<p>它也不是Perl, Java, C++或者Haskell。当你把其它语言的使用经验用在python上时要当心。一个简单的例子可以说明：</p>

<pre><code>% timeit.py -s 'x = 47' 'x * 2'
1000000 loops, best of 3: 0.574 usec per loop
% timeit.py -s 'x = 47' 'x &lt;&lt; 1'
1000000 loops, best of 3: 0.524 usec per loop
% timeit.py -s 'x = 47' 'x + x'
1000000 loops, best of 3: 0.382 usec per loop
</code></pre>

<p>接下来考虑这段相同的C代码(仅仅“相加”的版本列在下面)：</p>

<pre><code>#include &lt;stdio.h&gt;

int main (int argc, char *argv[]) {
    int i = 47;
    int loop;
    for (loop=0; loop&lt;500000000; loop++)
        i + i;
    return 0;
}
</code></pre>

<p>执行时间：</p>

<pre><code>% for prog in mult add shift ; do
&lt; for i in 1 2 3 ; do
&lt; echo -n "$prog: "
&lt; /usr/bin/time ./$prog
&lt; done
&lt; echo
&lt; done
mult: 6.12 real 5.64 user 0.01 sys
mult: 6.08 real 5.50 user 0.04 sys
mult: 6.10 real 5.45 user 0.03 sys

add: 6.07 real 5.54 user 0.00 sys
add: 6.08 real 5.60 user 0.00 sys
add: 6.07 real 5.58 user 0.01 sys

shift: 6.09 real 5.55 user 0.01 sys
shift: 6.10 real 5.62 user 0.01 sys
shift: 6.06 real 5.50 user 0.01 sys
</code></pre>

<p>请注意在python中，把一个数自己加到自己身上比乘以2或是左移2位要快得多。但在所有的现代计算机体系结构的C语言中，上述3种算术操作都被翻译为了一个可以在一个时钟周期内执行的机器指令，所以你用哪种方式根本无所谓。</p>

<p>有一个python新手经常做的“测试”就是翻译这个Perl idiom：</p>

<pre><code>while (&lt;&gt;) {
    print;
}
</code></pre>

<p>在python中这就像是：</p>

<pre><code>import fileinput

for line in fileinput.input():
    print line,
</code></pre>

<p>然后以此来总结出python比perl慢许多的结论。哪怕其他人指出过许多次，python在一些事情上比perl慢但其它事情较快。相对的性能也常常取决于你对这两种语言的经验。</p>

<h3>用xrange代替range</h3>

<blockquote><p>如果你使用python3，这小节的内容就不再适用，因为python3里面range提供一个可以遍历任意长度的范围的iterator，而xrange不再存在。
python有2种方式取得一个数字范围：range和xrange。大多数人知道range，因为它提示性很强的名字。而xrange，以字母表排序排在很后面，所以知名度小得多。</p></blockquote>

<p>xrange是一个generator对象，基本上等同于下面这段python2.3代码：</p>

<pre><code>def xrange(start, stop=None, step=1):
    if stop is None:
        stop = start
        start = 0
    else:
        stop = int(stop)
    start = int(start)
    step = int(step)

    while start &lt; stop:
        yield start
        start += step
</code></pre>

<p>只不过它是纯C代码实现的。</p>

<p>xrange确实有一些限制。特别是，它只能和整型一起工作；你不能使用long或者float（它们会被转换成整型，就像上面展示的那样）。</p>

<p>不过它又确实，节省很多内存，并且除非你把产生的对象存在别的什么地方，否则在任何时候只会有一个被产生的对象存在。不同的地方是这样的：当你使用range时，它创建了一个包含整个范围的数字（int, long或者float）对象。所有这些对象在同一时间被创建，所有的对象都同时存在。这在数字的数量很大时是痛苦的事情。</p>

<p>另一方面，xrange不会马上创建任何数字 —— 仅仅创建range对象本身。仅当你拉动generator的时候才创建数字对象，例如，循环遍历它。比如：</p>

<pre><code>xrange(sys.maxint) # 没有循环，没有调用.next，所以没有任何数字被创建
</code></pre>

<p>因为这个原因，这段代码顺畅地运行。如果你用range来替换它，python就会卡住；要创建sys.maxint个数字对象（在典型的PC平台上大约21亿个）实在是忙得不能做其它事情。最终，它会耗尽内存并退出。</p>

<p>在python2.2以前的版本，xrange对象也支持一些优化，比如快速地是否成员判定(i in xrange(n))。这些特性由于很少使用而在2.2版本后被去掉。</p>

<h3>在运行时重新映射函数</h3>

<p>假如你有一个函数：</p>

<pre><code>class Test:
    def check(self,a,b,c):
        if a == 0:
            self.str = b*100
        else:
            self.str = c*100

a = Test()
def example():
    for i in xrange(0,100000):
        a.check(i,"b","c")

import profile
profile.run("example()")
</code></pre>

<p>而且假设这个函数被别的地方调用了很多次。</p>

<p>好了，除了第一次运行以外，你的check方法在后面的每次运行都会有一个if语句来拖慢你，所以你可以这样做：</p>

<pre><code>class Test2:
    def check(self,a,b,c):
        self.str = b*100
        self.check = self.check_post
    def check_post(self,a,b,c):
        self.str = c*100

a = Test2()
def example2():
    for i in xrange(0,100000):
        a.check(i,"b","c")

import profile
profile.run("example2()")
</code></pre>

<p>当然，这个例子还很不充分，不过当这个&#8217;if&#8217;语句是一个相当复杂的表达式(或包含许多&#8217;.&#8217;)，你就能尽量避免执行它们，如果你事先知道仅仅只有第一次为true的话。</p>

<h3>性能优化</h3>

<p>优化你的代码的第一步是查出哪里是瓶颈。对从来不会运行的代码或本来就很快的代码做优化是没有道理的。我使用2个模块来帮助找出我代码中的热点，profile和trace。在后面的例子里我还会使用timeit模块，这是python2.3新加入的。</p>

<blockquote><p>请参考单独的<a href="http://wiki.python.org/moin/PythonSpeed/Profiling">profiling文档</a>来查看除下文中的使用方式以外的其它使用方式。</p></blockquote>

<h4>Profiling</h4>

<p>在python的发行版中包含有好几个<a href="http://docs.python.org/library/profile.html">profiling模块</a>。用它们其中的一个来监控一些函数的性能是很容易的。假设你的主函数叫做&#8217;main&#8217;，不接受任何参数而且你现在想用profile模块来监控并执行它。最简单的方式就是你直接执行</p>

<pre><code>import profile
profile.run('main()')
</code></pre>

<p>当main()返回时，profile模块会打印出一个函数调用和对应执行时间的表格。输出结果能够被这个模块里面包含的一个叫Stats的类处理。从python2.4开始profile允许python内置方法和扩展模块的方法被监控性能。</p>

<p>另一个稍微长一些的关于使用profile和pstats模块进行性能监控的描述在这里（归档的版本）：</p>

<p><a href="http://web.archive.org/web/20060506162444/http://wingware.com/doc/howtos/performance-profiling-python-code">http://web.archive.org/web/20060506162444/http://wingware.com/doc/howtos/performance-profiling-python-code</a></p>

<h4>cProfile和Hotshot模块</h4>

<p>从python2.2开始，<a href="http://www.python.org/doc/current/lib/module-hotshot.html">hotshot包</a>就作为profile模块的替代品出现，虽然目前cProfile模块是比hotshot更加被推荐的选择。这些模块都是用C写的，所以使用hotshot(或cProfile)造成的性能影响应该很小，从而性能监控得更精确。在发行版的Tools/scripts文件夹下还有一个hotshotmain.py程序来方便你在命令行使用hotshot来监控你的程序。</p>

<h4>Trace模块</h4>

<p><a href="http://www.python.org/doc/current/lib/module-trace.html">trace模块</a>这一小节是从我原本写的profile模块那一小节里面拆分出来的，用作一些原始的语句级别的测试覆盖的内容。它自从我最初写它的时候开始以来已经被好几个人改了很多次。在python2.0里面你应该能在Tools/scripts里面找到trace.py。从python2.3开始它就被放到了标准库里面(Lib文件夹)。你可以把它拷到本地的bin文件夹里面并且设置好执行权限，然后直接运行它。这样从命令行直接监控整个脚本很方便：</p>

<pre><code>% trace.py -t spam.py eggs
</code></pre>

<p>在python2.4里甚至更简单。直接运行 python -m trace。</p>

<p>没有单独存在的文档，但你可以运行&#8221;pydoc trace&#8221;来查看内置的文档。</p>

<h4>让性能监控结果可视化</h4>

<p><a href="http://www.vrplumber.com/programming/runsnakerun/">RunSnakeRun</a>是一个由Mike Fletcher创造的图形化工具，能使cProfile输出的profile dump可视化。函数/方法调用可根据多种条件排序，源代码能在图形旁边展示，并带有调用统计。</p>

<p>一个示例：</p>

<pre><code>runsnake some_profile_dump.prof
</code></pre>

<p><a href="http://code.google.com/p/jrfonseca/wiki/Gprof2Dot">Gprof2Dot</a>是一个基于python的工具，能够将性能监控结果输出转换成图片，之后能够被转换成PNG图片或SVG。</p>

<p>在python2.5下一个典型的性能诊断过程看起来像下面这样（在老一些的平台上需要使用真正的脚本来替代 -m 选项）：</p>

<pre><code>python -m cProfile -o stat.prof MYSCRIPY.PY [ARGS...]
python -m pbp.scripts.gprof2dot -f pstats -o stat.dot stat.prof
dot -ostat.png -Tpng stat.dot
</code></pre>

<p><a href="http://pycallgraph.slowchop.com/">PyCallGraph</a> pycallgraph是一个能够为python程序创建调用图python模块。它生成一个展示一个模块的函数调用以及他们对其它函数的调用的PNG图片文件，还包含函数被调用的次数和消耗的时间。</p>

<p>典型的使用：</p>

<pre><code>pycallgraph scriptname.py
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[翻译]"AI-class"中的概率论必备知识]]></title>
    <link href="http://pfmiles.github.com/blog/theory-of-probability-required-in-ai-class"/>
    <updated>2011-11-01T13:35:00+08:00</updated>
    <id>http://pfmiles.github.com/blog/theory-of-probability-required-in-ai-class</id>
    <content type="html"><![CDATA[<p><strong>在2011年末的ai-class中出现许多关于概率论的问题，其实这里有一篇神帖非常精彩：<a href="http://www.reddit.com/r/aiclass/comments/lpw21/a_challenge_to_the_over_achievers_who_really/">A challenge to the Over achievers who really understand Probability</a> 精华在于评论</strong></p>

<p>总结起来，ai-class中所用到的概率论，可以总结为下面几条：</p>

<ol>
<li>P(A,B) = P(A|B)P(B)              // bayes theorem ie; conditional probability</li>
<li>P(A) = P(A,B&#8217;) + P(A,B)          // total probability : let B&#8217; = not B</li>
<li>P(A) = P(A|B)P(B) + P(A|B&#8217;)P(B&#8217;) // total probability</li>
<li>P(A1,A2,A3,A4) = P(A1|A2,A3,A4)P(A2|A3,A4)P(A3|A4)P(A4) // chain rule - can be extended to n variables</li>
</ol>


<p>第一个，说是贝叶斯公式，可能一眼看不出来，但这正是其神奇之处，只要稍加变化:</p>

<pre><code>P(A, B) = P(A|B)P(B) --&gt; 
P(B|A)P(A) = P(A|B)P(B) --&gt; 
P(A|B) = P(B|A)P(A) / P(B)    // 这就是“正规”的贝叶斯公式
</code></pre>

<p>个人认为，比起“正规”的贝叶斯公式，第一个这种变种会好记得多，并且和“条件概率”结合起来，加深理解了贝叶斯公式的本质(本质上它就是条件概率的巧妙关系)；</p>

<p>第二、三个，都是“全概率公式”，只不过第三个比第二个多变化了一次而已(用条件概率公式变化了一下)；</p>

<p>第四个，是“链条法则”，其实一步一步自己慢慢利用上述的其它公式也能变幻出这个结果，但每次都这么做过于繁复，所以直接当作一个深刻的结论记下来会有很大好处。</p>

<p>OK，上面这些总结足够——秒杀ai-class里面的任何一个概率论问题，主要用在贝叶斯网络里面；包括后面开课的<a href="http://www.udacity.com/overview/Course/cs373">CS 373: PROGRAMMING A ROBOTIC CAR</a>，所有的概率论相关计算都能通过上述总结来导出；
一般过程就是：将原公式通过上述总结进行变换，向已知条件“靠拢”，最后求值；</p>

<p>当然, 还有一小部分内容叫做<strong>“D-Separation”</strong>，懂得了它之后，才敢说“秒杀”贝叶斯网络，这是后面需要介绍的一点点内容: (待续&#8230;)</p>

<!--more-->

]]></content>
  </entry>
  
</feed>
